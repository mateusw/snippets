{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475e663a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2025 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cl9i08h07sm",
   "metadata": {},
   "source": [
    "# Parking Lot Analysis Pipeline\n",
    "> **Note**: This notebook is for reference and educational purposes only. Not intended for production use.  \n",
    "> **Questions?** mateuswagner at google.com\n",
    "## Overview\n",
    "This notebook implements an automated parking lot analysis system using a two-stage approach:\n",
    "\n",
    "1. **Image Annotation** - Uses Google Gemini's vision model to analyze parking lot aerial images and place magenta dots above cars and empty spaces\n",
    "2. **Dot Detection** - Applies computer vision techniques (HSV color filtering, morphological operations) to detect and count the magenta markers\n",
    "3. **Visualization** - Generates multi-panel visualizations showing detection results and summary statistics\n",
    "\n",
    "## Workflow\n",
    "```\n",
    "Input Images → Gemini Processing → Annotated Images → OpenCV Detection → Count & Visualization\n",
    "```\n",
    "\n",
    "## Requirements\n",
    "- Google Cloud Vertex AI access with Gemini API enabled\n",
    "- Input images in `../demo-images/` directory\n",
    "- Output saved to `../outputs/processed/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrmev4xdzz",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cff8434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# - google-genai: Google Generative AI SDK for Gemini API access\n",
    "# - opencv-python: Computer vision library for image processing\n",
    "# - numpy: Numerical operations for array manipulation\n",
    "# - matplotlib: Visualization library for plotting results\n",
    "!pip install --upgrade google-genai opencv-python numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85685b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Google Generative AI ---\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# --- Standard Library ---\n",
    "import base64\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# --- Computer Vision & Data Processing ---\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9e61af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Gemini client using Vertex AI authentication\n",
    "# Requires: GOOGLE_CLOUD_PROJECT environment variable or gcloud auth\n",
    "client = genai.Client(vertexai=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hzm01nrwi9",
   "metadata": {},
   "source": [
    "## 2. Image Annotation with Gemini\n",
    "\n",
    "This section sends parking lot aerial images to Google Gemini, which analyzes the scene and returns annotated images with magenta dots placed above:\n",
    "- **Occupied spaces** (cars, trucks, vehicles)\n",
    "- **Empty parking spaces**\n",
    "- **Obstructions** (shopping carts, garbage containers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a340e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Directory paths\n",
    "input_dir = Path(\"../demo-images\")          # Source images to process\n",
    "output_dir = Path(\"../outputs/processed\")   # Destination for annotated images\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Generate unique timestamp for this batch run\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# =============================================================================\n",
    "# GEMINI MODEL CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Model selection - using Gemini 3 Pro with image generation capability\n",
    "model = \"gemini-3-pro-image-preview\"\n",
    "\n",
    "# Prompt instructs Gemini to annotate parking spaces with magenta markers\n",
    "prompt = \"\"\"\n",
    "Place a small magenta dot (255, 0, 255) above each car and each empty parking space within the parking lots boundaries.\n",
    "Also consider spaces occupied by garbage containers and shopping carts.\n",
    "\"\"\"\n",
    "\n",
    "# Generation parameters for image output\n",
    "generate_content_config = types.GenerateContentConfig(\n",
    "    temperature=1,              # Controls randomness (1 = more creative)\n",
    "    top_p=0.95,                 # Nucleus sampling threshold\n",
    "    max_output_tokens=32768,    # Maximum response length\n",
    "    response_modalities=[\"TEXT\", \"IMAGE\"],  # Enable both text and image output\n",
    "    \n",
    "    # Disable safety filters for aerial imagery processing\n",
    "    safety_settings=[\n",
    "        types.SafetySetting(category=\"HARM_CATEGORY_HATE_SPEECH\", threshold=\"OFF\"),\n",
    "        types.SafetySetting(category=\"HARM_CATEGORY_DANGEROUS_CONTENT\", threshold=\"OFF\"),\n",
    "        types.SafetySetting(category=\"HARM_CATEGORY_SEXUALLY_EXPLICIT\", threshold=\"OFF\"),\n",
    "        types.SafetySetting(category=\"HARM_CATEGORY_HARASSMENT\", threshold=\"OFF\")\n",
    "    ],\n",
    "    \n",
    "    # Output image configuration\n",
    "    image_config=types.ImageConfig(\n",
    "        aspect_ratio=\"1:1\",             # Square output\n",
    "        image_size=\"1K\",                # 1024x1024 resolution\n",
    "        output_mime_type=\"image/png\",   # PNG format for lossless quality\n",
    "    ),\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# IMAGE COLLECTION\n",
    "# =============================================================================\n",
    "\n",
    "# Gather all supported image formats from input directory\n",
    "image_files = (\n",
    "    sorted(input_dir.glob(\"*.png\")) + \n",
    "    sorted(input_dir.glob(\"*.jpg\")) + \n",
    "    sorted(input_dir.glob(\"*.jpeg\"))\n",
    ")\n",
    "\n",
    "print(f\"Found {len(image_files)} images to process\")\n",
    "print(f\"Batch timestamp: {timestamp}\")\n",
    "print(f\"Output directory: {output_dir}\\n\")\n",
    "\n",
    "# =============================================================================\n",
    "# BATCH PROCESSING LOOP\n",
    "# =============================================================================\n",
    "\n",
    "for img_path in image_files:\n",
    "    print(f\"Processing: {img_path.name}\")\n",
    "    \n",
    "    # Load image as binary data\n",
    "    with open(img_path, \"rb\") as f:\n",
    "        image_data = f.read()\n",
    "    \n",
    "    # Determine MIME type based on file extension\n",
    "    mime_type = \"image/png\" if img_path.suffix.lower() == \".png\" else \"image/jpeg\"\n",
    "    \n",
    "    # Construct multimodal request with image + text prompt\n",
    "    msg_image = types.Part.from_bytes(data=image_data, mime_type=mime_type)\n",
    "    msg_text = types.Part.from_text(text=prompt)\n",
    "    \n",
    "    contents = [\n",
    "        types.Content(role=\"user\", parts=[msg_image, msg_text]),\n",
    "    ]\n",
    "    \n",
    "    # Send request to Gemini API\n",
    "    response = client.models.generate_content(\n",
    "        model=model,\n",
    "        contents=contents,\n",
    "        config=generate_content_config\n",
    "    )\n",
    "    \n",
    "    # Extract and save the annotated image from response\n",
    "    if response.candidates and response.candidates[0].content.parts:\n",
    "        for part in response.candidates[0].content.parts:\n",
    "            if hasattr(part, 'inline_data') and part.inline_data:\n",
    "                # Save with timestamped filename to avoid overwrites\n",
    "                output_filename = f\"{img_path.stem}_processed_{timestamp}{img_path.suffix}\"\n",
    "                output_path = output_dir / output_filename\n",
    "                \n",
    "                with open(output_path, \"wb\") as f:\n",
    "                    f.write(part.inline_data.data)\n",
    "                print(f\"  Saved: {output_path}\")\n",
    "                \n",
    "            elif hasattr(part, 'text') and part.text:\n",
    "                # Log any text responses from the model\n",
    "                print(f\"  Model response: {part.text}\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Rate limiting: 10-second delay between requests to avoid API throttling\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dxo35551jwq",
   "metadata": {},
   "source": [
    "## 3. Magenta Dot Detection\n",
    "\n",
    "This section uses OpenCV to detect and count the magenta dots placed by Gemini. The detection pipeline:\n",
    "\n",
    "1. **Color Space Conversion** - Convert BGR to HSV for robust color detection\n",
    "2. **HSV Thresholding** - Isolate magenta pixels using narrow hue range (H: 145-165)\n",
    "3. **Morphological Cleaning** - Remove noise with opening/closing operations\n",
    "4. **Contour Analysis** - Find connected components and filter by area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cfbe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate all processed images from the Gemini annotation step\n",
    "# Uses the batch timestamp to match files from the current run\n",
    "processed_images = (\n",
    "    sorted(output_dir.glob(f\"*_processed_{timestamp}*.png\")) +\n",
    "    sorted(output_dir.glob(f\"*_processed_{timestamp}*.jpg\"))\n",
    ")\n",
    "\n",
    "print(f\"Found {len(processed_images)} annotated images to analyze\")\n",
    "print(f\"Source directory: {output_dir}\\n\")\n",
    "\n",
    "# Initialize storage for detection results across all images\n",
    "all_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89597b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DETECTION PARAMETERS\n",
    "# =============================================================================\n",
    "\n",
    "# HSV color range for magenta detection\n",
    "# Magenta in HSV: Hue ~300 degrees = 150 in OpenCV's 0-180 scale\n",
    "# Using narrow range to avoid false positives from similar colors\n",
    "LOWER_MAGENTA = np.array([145, 150, 150])  # [Hue, Saturation, Value] minimum\n",
    "UPPER_MAGENTA = np.array([165, 255, 255])  # [Hue, Saturation, Value] maximum\n",
    "\n",
    "# Contour area thresholds (in pixels) to filter valid dots\n",
    "MIN_DOT_AREA = 10   # Dots smaller than this are likely noise\n",
    "MAX_DOT_AREA = 60   # Dots larger than this are likely artifacts\n",
    "\n",
    "# Morphological kernel sizes for noise reduction\n",
    "KERNEL_OPEN = np.ones((3, 3), np.uint8)   # For removing small noise (opening)\n",
    "KERNEL_CLOSE = np.ones((5, 5), np.uint8)  # For filling small gaps (closing)\n",
    "\n",
    "# =============================================================================\n",
    "# DETECTION LOOP\n",
    "# =============================================================================\n",
    "\n",
    "for image_path in processed_images:\n",
    "    print(f\"Analyzing: {image_path.name}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Load the annotated image\n",
    "    img = cv2.imread(str(image_path))\n",
    "    if img is None:\n",
    "        print(f\"  ERROR: Failed to load image\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"  Dimensions: {img.shape[1]}x{img.shape[0]} pixels\")\n",
    "    \n",
    "    # --- Step 1: Convert to HSV color space ---\n",
    "    # HSV provides better color separation than BGR for thresholding\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    # --- Step 2: Create binary mask for magenta pixels ---\n",
    "    mask = cv2.inRange(hsv, LOWER_MAGENTA, UPPER_MAGENTA)\n",
    "    \n",
    "    # --- Step 3: Apply morphological operations to clean the mask ---\n",
    "    # Opening: Erode then dilate - removes small noise spots\n",
    "    mask_cleaned = cv2.morphologyEx(mask, cv2.MORPH_OPEN, KERNEL_OPEN, iterations=1)\n",
    "    # Closing: Dilate then erode - fills small holes in detected regions\n",
    "    mask_cleaned = cv2.morphologyEx(mask_cleaned, cv2.MORPH_CLOSE, KERNEL_CLOSE, iterations=1)\n",
    "    \n",
    "    # --- Step 4: Find contours (connected components) ---\n",
    "    contours, _ = cv2.findContours(mask_cleaned, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # --- Step 5: Filter contours by area to identify valid dots ---\n",
    "    valid_contours = [\n",
    "        cnt for cnt in contours \n",
    "        if MIN_DOT_AREA <= cv2.contourArea(cnt) <= MAX_DOT_AREA\n",
    "    ]\n",
    "    \n",
    "    dot_count = len(valid_contours)\n",
    "    areas = [cv2.contourArea(cnt) for cnt in valid_contours]\n",
    "    \n",
    "    # --- Report results ---\n",
    "    print(f\"  Raw contours detected: {len(contours)}\")\n",
    "    print(f\"  Valid dots (area {MIN_DOT_AREA}-{MAX_DOT_AREA}px): {dot_count}\")\n",
    "    \n",
    "    if areas:\n",
    "        print(f\"  Dot area stats: mean={np.mean(areas):.1f}, std={np.std(areas):.1f}, \"\n",
    "              f\"range=[{min(areas):.0f}, {max(areas):.0f}]\")\n",
    "    \n",
    "    # Store results for visualization\n",
    "    all_results.append({\n",
    "        'filename': image_path.name,\n",
    "        'image': img,\n",
    "        'mask': mask_cleaned,\n",
    "        'contours': valid_contours,\n",
    "        'dot_count': dot_count,\n",
    "        'areas': areas\n",
    "    })\n",
    "    \n",
    "    print(f\"  TOTAL PARKING SPACES: {dot_count}\\n\")\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "print(\"=\" * 50)\n",
    "print(f\"Detection complete: {len(all_results)} images analyzed\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xsccnm9ca8",
   "metadata": {},
   "source": [
    "## 4. Visualization and Results\n",
    "\n",
    "Generate diagnostic visualizations showing:\n",
    "- Original annotated image from Gemini\n",
    "- Binary detection mask after HSV thresholding\n",
    "- Detected dots highlighted with green contours\n",
    "- Isolated magenta pixels for verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ui5baksya5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GENERATE MULTI-PANEL VISUALIZATIONS\n",
    "# =============================================================================\n",
    "\n",
    "for result in all_results:\n",
    "    # Unpack result data\n",
    "    img = result['image']\n",
    "    mask = result['mask']\n",
    "    contours = result['contours']\n",
    "    dot_count = result['dot_count']\n",
    "    filename = result['filename']\n",
    "    \n",
    "    # Create 2x2 subplot figure\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle(f\"Detection Results: {filename}\", fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # --- Panel 1: Original annotated image ---\n",
    "    axes[0, 0].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    axes[0, 0].set_title('Gemini-Annotated Image', fontsize=12)\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    # --- Panel 2: Binary detection mask ---\n",
    "    axes[0, 1].imshow(mask, cmap='gray')\n",
    "    axes[0, 1].set_title('Magenta Detection Mask (HSV Threshold)', fontsize=12)\n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    # --- Panel 3: Detected dots with contour overlay ---\n",
    "    detection_overlay = img.copy()\n",
    "    cv2.drawContours(detection_overlay, contours, -1, (0, 255, 0), 2)  # Green contours\n",
    "    cv2.putText(\n",
    "        detection_overlay, \n",
    "        f'Detected: {dot_count} dots', \n",
    "        (10, 50), \n",
    "        cv2.FONT_HERSHEY_SIMPLEX, \n",
    "        1.5, \n",
    "        (0, 255, 0), \n",
    "        3\n",
    "    )\n",
    "    axes[1, 0].imshow(cv2.cvtColor(detection_overlay, cv2.COLOR_BGR2RGB))\n",
    "    axes[1, 0].set_title(f'Detected Dots: {dot_count}', fontsize=12)\n",
    "    axes[1, 0].axis('off')\n",
    "    \n",
    "    # --- Panel 4: Isolated magenta pixels ---\n",
    "    isolated_magenta = cv2.bitwise_and(img, img, mask=mask)\n",
    "    axes[1, 1].imshow(cv2.cvtColor(isolated_magenta, cv2.COLOR_BGR2RGB))\n",
    "    axes[1, 1].set_title('Isolated Magenta Pixels', fontsize=12)\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save visualization to output directory\n",
    "    viz_filename = filename.replace('_processed_', '_detection_')\n",
    "    viz_path = output_dir / viz_filename\n",
    "    plt.savefig(viz_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"Saved visualization: {viz_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# FINAL SUMMARY REPORT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PARKING LOT ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "total_dots = sum(r['dot_count'] for r in all_results)\n",
    "print(f\"\\nTotal images analyzed: {len(all_results)}\")\n",
    "print(f\"Total parking spaces detected: {total_dots}\")\n",
    "\n",
    "if all_results:\n",
    "    avg_dots = total_dots / len(all_results)\n",
    "    print(f\"Average spaces per image: {avg_dots:.1f}\")\n",
    "    \n",
    "    print(\"\\nBreakdown by image:\")\n",
    "    print(\"-\" * 40)\n",
    "    for r in all_results:\n",
    "        print(f\"  {r['filename']}: {r['dot_count']} spaces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcd4da2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
