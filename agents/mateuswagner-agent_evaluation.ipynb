{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2025 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "# ADK Agent Evaluation with Vertex AI Gen AI Evaluation Service\n",
    "\n",
    "> **Note**: This notebook is for reference and educational purposes only. Not intended for production use.  \n",
    "> **Questions?** mateuswagner@google.com\n",
    "\n",
    "This notebook demonstrates how to build and systematically evaluate AI agents using Google's Agent Development Kit (ADK) and Vertex AI Gen AI Evaluation Service.\n",
    "\n",
    "## What You'll Build\n",
    "\n",
    "A product research agent that retrieves product details and prices, evaluated using three complementary approaches to measure quality, tool usage, and business logic compliance.\n",
    "\n",
    "**Tech Stack**: ADK + Gemini + Vertex AI Gen AI Evaluation Service\n",
    "\n",
    "## Evaluation Approaches\n",
    "\n",
    "### 1. Rubric-Based Metrics (LLM-as-Judge)\n",
    "\n",
    "Uses a separate LLM judge to evaluate response quality with pre-built metrics:\n",
    "- `instruction_following` - Answers the question asked\n",
    "- `fluency` - Grammatically correct and natural\n",
    "- `coherence` - Logically consistent and clear\n",
    "- `safety` - Free from harmful content\n",
    "- `text_quality` - Overall quality assessment\n",
    "- `verbosity` - Appropriate response length\n",
    "\n",
    "**When to use**: Automated quality assurance for customer-facing agent responses\n",
    "\n",
    "### 2. **Custom** Metrics (LLM-as-Judge)\n",
    "\n",
    "Evaluates domain-specific criteria using custom prompt templates:\n",
    "\n",
    "**When to use**: Business-specific quality validation beyond generic metrics\n",
    "\n",
    "### 3. Custom Function Metrics (Deterministic)\n",
    "\n",
    "Custom Python functions that validate agent behavior without Judges calls:\n",
    "\n",
    "**Tool Usage Validation**:\n",
    "- `tool_count` - Number of tools called\n",
    "- `tool_efficiency` - Uses minimum necessary tools\n",
    "\n",
    "**Response Validation**:\n",
    "- `response_length` - Character count\n",
    "- `response_conciseness` - Appropriately brief\n",
    "- `numeric_response` - Contains numbers when expected\n",
    "\n",
    "**Business Logic Validation**:\n",
    "- `valid_product` - Queries only catalog products\n",
    "- `correct_tool_selection` - Uses appropriate tool for query type\n",
    "- `price_range_validation` - Price within expected bounds\n",
    "\n",
    "**When to use**: Fast, deterministic validation of agent logic and compliance\n",
    "\n",
    "## Implementation Workflow\n",
    "\n",
    "**Setup**\n",
    "\n",
    "1. Install dependencies and configure GCP project\n",
    "2. Initialize Vertex AI with experiment tracking\n",
    "3. Define helper functions for parsing and display\n",
    "\n",
    "**Agent Development**\n",
    "\n",
    "4. Create custom tools for product research\n",
    "5. Configure Gemini model and build ADK agent\n",
    "6. Create evaluation dataset with expected behaviors\n",
    "\n",
    "**Evaluation**\n",
    "\n",
    "7. Run rubric-based evaluation for response quality\n",
    "8. Run custom pointwise evaluation for completeness\n",
    "9. Run function-based evaluation for logic validation\n",
    "10. Compare metrics across evaluation approaches\n",
    "11. Review detailed results and summary statistics\n",
    "\n",
    "## Evaluation Dataset Structure\n",
    "\n",
    "The dataset serves as ground truth for measuring agent performance.\n",
    "\n",
    "This enables:\n",
    "- Trajectory validation against expected tool usage\n",
    "- Regression testing for agent updates\n",
    "- Systematic coverage of edge cases\n",
    "- Objective baselines for automated metrics\n",
    "\n",
    "## Evaluation Modes\n",
    "\n",
    "**On-the-fly generation** (used in this notebook): Provide only prompts, agent generates responses during evaluation\n",
    "\n",
    "**BYOD (Bring Your Own Data)**: Provide prompts, responses, and trajectories for faster evaluation of pre-generated results\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "**LLM-as-Judge**: Uses a separate model to evaluate response quality based on rubrics and criteria. Provides nuanced assessment but has API costs.\n",
    "\n",
    "**Deterministic Metrics**: Python functions that validate specific behaviors. Fast, free, and reproducible but limited to predefined rules.\n",
    "\n",
    "**Complementary Strategy**: Combine both approaches for comprehensive agent evaluation - quality assessment via LLM judges and logic validation via deterministic functions.\n",
    "\n",
    "## Outputs\n",
    "\n",
    "All evaluation results are tracked in Vertex AI Experiments and persisted to Cloud Storage for reproducibility and comparison across runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61RBz8LLbxCR"
   },
   "source": [
    "## Get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "No17Cw5hgx12"
   },
   "outputs": [],
   "source": [
    "## Installation\n",
    "# Python 3.10+ recommended\n",
    "# Virtual environment (venv) recommended for isolation\n",
    "# Google Cloud SDK initialized: `gcloud init`\n",
    "\n",
    "%pip install --upgrade --quiet 'google-adk' nbformat 'google-cloud-aiplatform[evaluation]'\n",
    "\n",
    "# Restart your Jupyter kernel !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5303c05f7aa6"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6fc324893334"
   },
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "import warnings\n",
    "from typing import Any\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "from IPython.display import HTML, Markdown, display\n",
    "\n",
    "# Google Cloud imports\n",
    "import vertexai\n",
    "from google.cloud import aiplatform\n",
    "from google.genai import types\n",
    "from google.cloud import storage\n",
    "from google.api_core import exceptions\n",
    "\n",
    "# Google ADK imports\n",
    "from google.adk.agents import Agent\n",
    "from google.adk.events import Event\n",
    "from google.adk.runners import Runner\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "\n",
    "# Vertex AI Evaluation imports\n",
    "from vertexai.preview.evaluation import EvalTask\n",
    "from vertexai.preview.evaluation.metrics import (\n",
    "    CustomMetric,\n",
    "    PointwiseMetric,\n",
    "    PointwiseMetricPromptTemplate,\n",
    "    TrajectorySingleToolUse,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONFIGURATION SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Nqwi-5ufWp_B"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-947f4515-5d4f-4d0a-ac64-8ddcfb959171\" href=\"#view-view-vertex-resource-947f4515-5d4f-4d0a-ac64-8ddcfb959171\">\n",
       "          <span class=\"material-icons view-vertex-icon\">science</span>\n",
       "          <span>View Experiment</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-947f4515-5d4f-4d0a-ac64-8ddcfb959171');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/evaluate-adk-agent/runs?project=matt-demos');\n",
       "              } else {\n",
       "                window.open('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/evaluate-adk-agent/runs?project=matt-demos', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Enable Vertex AI for Google Gen AI SDK\n",
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"true\"\n",
    "\n",
    "# Google Cloud Project Configuration\n",
    "# CHANGE THIS: Set your Google Cloud project ID\n",
    "PROJECT_ID = \"matt-demos\" # CHANGE IT!\n",
    "\n",
    "# Fallback to environment variable if not set\n",
    "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
    "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
    "\n",
    "# Default region for Vertex AI resources\n",
    "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")\n",
    "\n",
    "# Cloud Storage Configuration\n",
    "# CHANGE THIS: Set your Cloud Storage bucket name\n",
    "BUCKET_NAME = \"53642dcf-cdb9-4f6c-a3bb-cf6595602893\"\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
    "os.environ[\"GOOGLE_CLOUD_LOCATION\"] = LOCATION\n",
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"True\"\n",
    "\n",
    "# Vertex AI Experiments name for tracking evaluation runs\n",
    "EXPERIMENT_NAME = \"evaluate-adk-agent\"\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Initialize Vertex AI with experiment tracking\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION, experiment=EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Bucket '53642dcf-cdb9-4f6c-a3bb-cf6595602893' already exists - skipping creation\n",
      "  Location: US | Created: 2025-10-17 18:15:55.580000+00:00\n"
     ]
    }
   ],
   "source": [
    "# OPTIONAL: Create GCS bucket (skip if already exists)\n",
    "\n",
    "def create_bucket_if_not_exists(bucket_name: str, location: str) -> None:\n",
    "    \"\"\"Create a GCS bucket if it doesn't already exist.\"\"\"\n",
    "    storage_client = storage.Client(project=PROJECT_ID)\n",
    "    \n",
    "    try:\n",
    "        bucket = storage_client.lookup_bucket(bucket_name)\n",
    "        \n",
    "        if bucket is not None:\n",
    "            print(f\"✓ Bucket '{bucket_name}' already exists - skipping creation\")\n",
    "            print(f\"  Location: {bucket.location} | Created: {bucket.time_created}\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Creating bucket '{bucket_name}' in {location}...\")\n",
    "        bucket = storage_client.create_bucket(bucket_name, location=location)\n",
    "        print(f\"✓ Successfully created bucket '{bucket_name}' at gs://{bucket_name}\")\n",
    "        \n",
    "    except exceptions.Forbidden as e:\n",
    "        print(f\"✗ Permission denied. Ensure you have 'storage.buckets.create' permission\")\n",
    "        raise\n",
    "    except exceptions.Conflict as e:\n",
    "        print(f\"✗ Bucket name already taken globally. Try a different name.\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {e}\")\n",
    "        raise\n",
    "\n",
    "try:\n",
    "    create_bucket_if_not_exists(BUCKET_NAME, LOCATION)\n",
    "except Exception:\n",
    "    print(\"\\nBucket creation failed. Continue if bucket exists or create manually\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVnBDX54gz7j"
   },
   "source": [
    "## Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uSgWjMD_g1_v"
   },
   "outputs": [],
   "source": [
    "def get_id(length: int = 8) -> str:\n",
    "    \"\"\"Generate a uuid of a specified length (default=8).\"\"\"\n",
    "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "\n",
    "def parse_adk_output_to_dictionary(events: list[Event], *, as_json: bool = False):\n",
    "    \"\"\"\n",
    "    Parse ADK event output into a structured dictionary format,\n",
    "    with the predicted trajectory dumped as a JSON string.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    final_response = \"\"\n",
    "    trajectory = []\n",
    "\n",
    "    for event in events:\n",
    "        if not getattr(event, \"content\", None) or not getattr(event.content, \"parts\", None):\n",
    "            continue\n",
    "        for part in event.content.parts:\n",
    "            if getattr(part, \"function_call\", None):\n",
    "                info = {\n",
    "                    \"tool_name\": part.function_call.name,\n",
    "                    \"tool_input\": dict(part.function_call.args),\n",
    "                }\n",
    "                if info not in trajectory:\n",
    "                    trajectory.append(info)\n",
    "            if event.content.role == \"model\" and getattr(part, \"text\", None):\n",
    "                final_response = part.text.strip()\n",
    "\n",
    "    if as_json:\n",
    "        trajectory_out = json.dumps(trajectory)\n",
    "    else:\n",
    "        trajectory_out = trajectory\n",
    "\n",
    "    return {\"response\": final_response, \"predicted_trajectory\": trajectory_out}\n",
    "\n",
    "\n",
    "def format_output_as_markdown(output: dict) -> str:\n",
    "    \"\"\"Convert the output dictionary to a formatted markdown string.\"\"\"\n",
    "    markdown = \"### AI Response\\n\" + output[\"response\"] + \"\\n\\n\"\n",
    "    if output[\"predicted_trajectory\"]:\n",
    "        markdown += \"### Function Calls\\n\"\n",
    "        for call in output[\"predicted_trajectory\"]:\n",
    "            markdown += f\"- **Function**: `{call['tool_name']}`\\n\"\n",
    "            markdown += \"  - **Arguments**\\n\"\n",
    "            for key, value in call[\"tool_input\"].items():\n",
    "                markdown += f\"    - `{key}`: `{value}`\\n\"\n",
    "    return markdown\n",
    "\n",
    "\n",
    "def display_eval_report(eval_result: pd.DataFrame) -> None:\n",
    "    \"\"\"Display the evaluation results.\"\"\"\n",
    "    display(Markdown(\"### Summary Metrics\"))\n",
    "    display(\n",
    "        pd.DataFrame(\n",
    "            eval_result.summary_metrics.items(), columns=[\"metric\", \"value\"]\n",
    "        )\n",
    "    )\n",
    "    if getattr(eval_result, \"metrics_table\", None) is not None:\n",
    "        display(Markdown(\"### Row‑wise Metrics\"))\n",
    "        display(eval_result.metrics_table.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDaa2Mtsifmq"
   },
   "source": [
    "## Build ADK agent\n",
    "\n",
    "Build your application using ADK, including the Gemini model and custom tools that you define.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHwShhpOitKp"
   },
   "source": [
    "### Set agent tools\n",
    "\n",
    "To start, set the tools that a customer support agent needs to do their job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gA2ZKvfeislw"
   },
   "outputs": [],
   "source": [
    "def get_product_details(product_name: str):\n",
    "    \"\"\"Gathers basic details about a product.\"\"\"\n",
    "    details = {\n",
    "        \"smartphone\": \"A cutting-edge smartphone with advanced camera features and lightning-fast processing.\",\n",
    "        \"usb charger\": \"A super fast and light usb charger\",\n",
    "        \"shoes\": \"High-performance running shoes designed for comfort, support, and speed.\",\n",
    "        \"headphones\": \"Wireless headphones with advanced noise cancellation technology for immersive audio.\",\n",
    "        \"speaker\": \"A voice-controlled smart speaker that plays music, sets alarms, and controls smart home devices.\",\n",
    "    }\n",
    "    return details.get(product_name, \"Product details not found.\")\n",
    "\n",
    "\n",
    "def get_product_price(product_name: str):\n",
    "    \"\"\"Gathers price about a product.\"\"\"\n",
    "    details = {\n",
    "        \"smartphone\": 500,\n",
    "        \"usb charger\": 10,\n",
    "        \"shoes\": 100,\n",
    "        \"headphones\": 50,\n",
    "        \"speaker\": 80,\n",
    "    }\n",
    "    return details.get(product_name, \"Product price not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4mk5XPui4Y1"
   },
   "source": [
    "### Set Agent the model\n",
    "\n",
    "Configure the Gemini model for your ADK agent. This notebook uses **`gemini-2.5-flash`** for fast, cost-effective function calling.\n",
    "\n",
    "**Model Selection Guidelines:**\n",
    "- **`gemini-2.5-flash`**: Fast responses, low cost, ideal for production agents with straightforward tool usage\n",
    "- **`gemini-2.5-pro`**: Higher reasoning capability, better for complex multi-step workflows and ambiguous queries\n",
    "\n",
    "See the [Gemini model documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models) for detailed performance benchmarks and pricing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BaYeo6K2i-w1"
   },
   "outputs": [],
   "source": [
    "model = \"gemini-2.5-flash\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNlAY9cojEWz"
   },
   "source": [
    "### Assemble the ADK Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "gD5OB44g4sc3"
   },
   "outputs": [],
   "source": [
    "async def agent_parsed_outcome(query):\n",
    "   # Session identifiers for tracking agent interactions\n",
    "   app_name = \"product_research_app\"\n",
    "   user_id = \"user_one\"\n",
    "   session_id = \"session_one\"\n",
    "   \n",
    "   # Create agent with dynamic instruction based on query\n",
    "   product_research_agent = Agent(\n",
    "       name=\"ProductResearchAgent\",\n",
    "       model=model,\n",
    "       description=\"An agent that performs product research.\",\n",
    "       instruction=f\"\"\"\n",
    "       Analyze this user request: '{query}'.\n",
    "       If the request is about price, use get_product_price tool.\n",
    "       Otherwise, use get_product_details tool to get product information.\n",
    "       \"\"\",\n",
    "       tools=[get_product_details, get_product_price],\n",
    "   )\n",
    "\n",
    "   # Initialize in-memory session storage\n",
    "   session_service = InMemorySessionService()\n",
    "   await session_service.create_session(\n",
    "       app_name=app_name, user_id=user_id, session_id=session_id\n",
    "   )\n",
    "\n",
    "   # Create runner to execute agent with session management\n",
    "   runner = Runner(\n",
    "       agent=product_research_agent, app_name=app_name, session_service=session_service\n",
    "   )\n",
    "\n",
    "   # Format query as user message and run agent asynchronously\n",
    "   content = types.Content(role=\"user\", parts=[types.Part(text=query)])\n",
    "   events = [event async for event in runner.run_async(user_id=user_id, session_id=session_id, new_message=content)]\n",
    "   \n",
    "   # Parse events into dictionary with response and tool calls\n",
    "   return parse_adk_output_to_dictionary(events)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent Wrapper for Vertex AI Evaluation Service\n",
    "# This function will be passed to EvalTask.evaluate(runnable=agent_parsed_outcome_sync)\n",
    "# Vertex AI will call it for each prompt in the evaluation dataset, automatically generating\n",
    "# responses and trajectories on-the-fly for metrics computation.\n",
    "\n",
    "def agent_parsed_outcome_sync(prompt: str):\n",
    "    result = asyncio.run(agent_parsed_outcome(prompt))\n",
    "    result[\"predicted_trajectory\"] = json.dumps(result[\"predicted_trajectory\"])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "lGb58OJkjUs9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['thought_signature', 'function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### AI Response\n",
       "High-performance running shoes designed for comfort, support, and speed.\n",
       "\n",
       "### Function Calls\n",
       "- **Function**: `get_product_details`\n",
       "  - **Arguments**\n",
       "    - `product_name`: `shoes`\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test the agent\n",
    "\n",
    "response = await agent_parsed_outcome(query=\"Get product details for shoes\")\n",
    "display(Markdown(format_output_as_markdown(response)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e43229f3ad4f"
   },
   "source": [
    "### Prepare Agent Evaluation Dataset\n",
    "\n",
    "The evaluation dataset serves as the **ground truth benchmark** for measuring agent performance. It contains:\n",
    "- **`prompt`**: Test queries that cover diverse agent scenarios (price lookups, detail requests, multi-step tasks)\n",
    "- **`predicted_trajectory`**: Expected tool call sequences that define correct agent behavior\n",
    "\n",
    "**Uses:**\n",
    "- **Trajectory Validation**: Compare actual vs expected tool usage to catch logic errors (wrong tools, missing steps, extra calls)\n",
    "- **Regression Testing**: Ensure agent improvements don't break existing functionality\n",
    "- **Coverage Analysis**: Systematically test edge cases and multi-tool workflows\n",
    "- **Baseline for Metrics**: Powers both automated metrics (tool selection, efficiency) and LLM-based judges (response quality)\n",
    "\n",
    "Without this dataset, you're evaluating in a vacuum with no objective standard for correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "fFf8uTdUiDt3"
   },
   "outputs": [],
   "source": [
    "eval_data = {\n",
    "    \"prompt\": [\n",
    "        \"Get price for smartphone\",\n",
    "        \"Get product details and price for headphones\",\n",
    "        \"Get details for usb charger\",\n",
    "        \"Get product details and price for shoes\",\n",
    "        \"Get product details for speaker?\",\n",
    "    ],\n",
    "    \"predicted_trajectory\": [\n",
    "        [\n",
    "            {\n",
    "                \"tool_name\": \"get_product_price\",\n",
    "                \"tool_input\": {\"product_name\": \"smartphone\"},\n",
    "            }\n",
    "        ],\n",
    "        [\n",
    "            {\n",
    "                \"tool_name\": \"get_product_details\",\n",
    "                \"tool_input\": {\"product_name\": \"headphones\"},\n",
    "            },\n",
    "            {\n",
    "                \"tool_name\": \"get_product_price\",\n",
    "                \"tool_input\": {\"product_name\": \"headphones\"},\n",
    "            },\n",
    "        ],\n",
    "        [\n",
    "            {\n",
    "                \"tool_name\": \"get_product_details\",\n",
    "                \"tool_input\": {\"product_name\": \"usb charger\"},\n",
    "            }\n",
    "        ],\n",
    "        [\n",
    "            {\n",
    "                \"tool_name\": \"get_product_details\",\n",
    "                \"tool_input\": {\"product_name\": \"shoes\"},\n",
    "            },\n",
    "            {\"tool_name\": \"get_product_price\", \"tool_input\": {\"product_name\": \"shoes\"}},\n",
    "        ],\n",
    "        [\n",
    "            {\n",
    "                \"tool_name\": \"get_product_details\",\n",
    "                \"tool_input\": {\"product_name\": \"speaker\"},\n",
    "            }\n",
    "        ],\n",
    "    ],\n",
    "}\n",
    "\n",
    "eval_sample_dataset = pd.DataFrame(eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval #1\n",
    "## Rubric-Based Evaluation (Model-Based Metrics)\n",
    "\n",
    "Evaluates the **quality of agent text responses** using an LLM as a judge. Unlike trajectory metrics that validate tool usage, these metrics assess communication quality.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "A separate judge LLM (Gemini) evaluates each response by analyzing:\n",
    "- User prompt\n",
    "- Agent response\n",
    "- Metric-specific rubric/criteria\n",
    "\n",
    "### Metrics (6 model-based)\n",
    "\n",
    "| Metric | Evaluation Criteria | Scale |\n",
    "|--------|---------------------|-------|\n",
    "| `instruction_following` | Answers the question asked | 1-5 |\n",
    "| `fluency` | Grammatically correct and natural | 1-5 |\n",
    "| `coherence` | Logically consistent and clear | 1-5 |\n",
    "| `safety` | Free from harmful content | 1 (safe) / 0 (unsafe) |\n",
    "| `text_quality` | Overall quality | 1-5 |\n",
    "| `verbosity` | Appropriate response length | -2 to +2 (0 = optimal) |\n",
    "\n",
    "### Evaluation Modes\n",
    "\n",
    "1. **On-the-fly (used here)**: Provide only `prompt` column, agent generates responses during evaluation\n",
    "\n",
    "### Output\n",
    "\n",
    "- **GCS**: `gs://{BUCKET_URI}/rubric-metric-eval/`\n",
    "- **Tracking**: Logged to Vertex AI Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-e4383ce1-0d67-49d6-9a8f-489f2383c84b\" href=\"#view-view-vertex-resource-e4383ce1-0d67-49d6-9a8f-489f2383c84b\">\n",
       "          <span class=\"material-icons view-vertex-icon\">science</span>\n",
       "          <span>View Experiment</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-e4383ce1-0d67-49d6-9a8f-489f2383c84b');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/evaluate-adk-agent/runs?project=matt-demos');\n",
       "              } else {\n",
       "                window.open('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/evaluate-adk-agent/runs?project=matt-demos', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Associating projects/941046250687/locations/us-central1/metadataStores/default/contexts/evaluate-adk-agent-rubric-metric-eval-x940vgl7 to Experiment: evaluate-adk-agent\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-dbb9644d-d17c-4be4-b5fe-c364b67621fb\" href=\"#view-view-vertex-resource-dbb9644d-d17c-4be4-b5fe-c364b67621fb\">\n",
       "          <span class=\"material-icons view-vertex-icon\">science</span>\n",
       "          <span>View Experiment Run</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-dbb9644d-d17c-4be4-b5fe-c364b67621fb');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/evaluate-adk-agent/runs/evaluate-adk-agent-rubric-metric-eval-x940vgl7?project=matt-demos');\n",
       "              } else {\n",
       "                window.open('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/evaluate-adk-agent/runs/evaluate-adk-agent-rubric-metric-eval-x940vgl7?project=matt-demos', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging Eval experiment evaluation metadata: {'output_file': 'gs://53642dcf-cdb9-4f6c-a3bb-cf6595602893/rubric-metric-eval/eval_results_2025-10-30-15-58-37-48036.csv'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]Warning: there are non-text parts in the response: ['thought_signature', 'function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "Warning: there are non-text parts in the response: ['thought_signature', 'function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "Warning: there are non-text parts in the response: ['thought_signature', 'function_call', 'function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "Warning: there are non-text parts in the response: ['thought_signature', 'function_call', 'function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "Warning: there are non-text parts in the response: ['thought_signature', 'function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "100%|██████████| 5/5 [00:02<00:00,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 5 responses are successfully generated from the runnable.\n",
      "Computing metrics with a total of 30 Vertex Gen AI Evaluation Service API requests.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:30<00:00,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 30 metric requests are successfully computed.\n",
      "Evaluation Took:30.81328302099996 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-06956e03-03aa-4a3f-afaa-8fc49746a9fc\" href=\"#view-view-vertex-resource-06956e03-03aa-4a3f-afaa-8fc49746a9fc\">\n",
       "          <span class=\"material-icons view-vertex-icon\">science</span>\n",
       "          <span>View Experiment</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-06956e03-03aa-4a3f-afaa-8fc49746a9fc');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/evaluate-adk-agent/runs?project=matt-demos');\n",
       "              } else {\n",
       "                window.open('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/evaluate-adk-agent/runs?project=matt-demos', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-8d85bd10-09f6-4059-9506-7b53c34c1485\" href=\"#view-view-vertex-resource-8d85bd10-09f6-4059-9506-7b53c34c1485\">\n",
       "          <span class=\"material-icons view-vertex-icon\">bar_chart</span>\n",
       "          <span>View evaluation results</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-8d85bd10-09f6-4059-9506-7b53c34c1485');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://console.cloud.google.com/storage/browser/_details/53642dcf-cdb9-4f6c-a3bb-cf6595602893/rubric-metric-eval/eval_results_2025-10-30-15-58-37-48036/eval_results_2025-10-30-15-58-37-48036.csv;colab_enterprise=gen_ai_evaluation');\n",
       "              } else {\n",
       "                window.open('https://console.cloud.google.com/storage/browser/_details/53642dcf-cdb9-4f6c-a3bb-cf6595602893/rubric-metric-eval/eval_results_2025-10-30-15-58-37-48036/eval_results_2025-10-30-15-58-37-48036.csv;colab_enterprise=gen_ai_evaluation', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Summary Metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>row_count</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>instruction_following/mean</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>instruction_following/std</td>\n",
       "      <td>1.414214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fluency/mean</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fluency/std</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>coherence/mean</td>\n",
       "      <td>4.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>coherence/std</td>\n",
       "      <td>1.788854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>safety/mean</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>safety/std</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>text_quality/mean</td>\n",
       "      <td>4.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>text_quality/std</td>\n",
       "      <td>1.303840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>verbosity/mean</td>\n",
       "      <td>-1.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>verbosity/std</td>\n",
       "      <td>0.547723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>latency_in_seconds/mean</td>\n",
       "      <td>2.366005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>latency_in_seconds/std</td>\n",
       "      <td>0.245962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>failure/mean</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>failure/std</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        metric     value\n",
       "0                    row_count  5.000000\n",
       "1   instruction_following/mean  4.000000\n",
       "2    instruction_following/std  1.414214\n",
       "3                 fluency/mean  4.000000\n",
       "4                  fluency/std  0.707107\n",
       "5               coherence/mean  4.200000\n",
       "6                coherence/std  1.788854\n",
       "7                  safety/mean  1.000000\n",
       "8                   safety/std  0.000000\n",
       "9            text_quality/mean  4.200000\n",
       "10            text_quality/std  1.303840\n",
       "11              verbosity/mean -1.400000\n",
       "12               verbosity/std  0.547723\n",
       "13     latency_in_seconds/mean  2.366005\n",
       "14      latency_in_seconds/std  0.245962\n",
       "15                failure/mean  0.000000\n",
       "16                 failure/std  0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Row‑wise Metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "      <th>latency_in_seconds</th>\n",
       "      <th>failure</th>\n",
       "      <th>predicted_trajectory</th>\n",
       "      <th>instruction_following/explanation</th>\n",
       "      <th>instruction_following/score</th>\n",
       "      <th>fluency/explanation</th>\n",
       "      <th>fluency/score</th>\n",
       "      <th>coherence/explanation</th>\n",
       "      <th>coherence/score</th>\n",
       "      <th>safety/explanation</th>\n",
       "      <th>safety/score</th>\n",
       "      <th>text_quality/explanation</th>\n",
       "      <th>text_quality/score</th>\n",
       "      <th>verbosity/explanation</th>\n",
       "      <th>verbosity/score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Get price for smartphone</td>\n",
       "      <td>The price for smartphone is 500.</td>\n",
       "      <td>2.279498</td>\n",
       "      <td>0</td>\n",
       "      <td>[{\"tool_name\": \"get_product_price\", \"tool_inpu...</td>\n",
       "      <td>The response directly addresses the instructio...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The response has a minor grammatical error due...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>The response is a single, clear, and direct st...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The response is safe as it contains no hate sp...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The response is coherent, fluent, and perfectl...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>The response is excessively brief and lacks cr...</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Get product details and price for headphones</td>\n",
       "      <td>Here are the details for headphones: Wireless ...</td>\n",
       "      <td>2.561855</td>\n",
       "      <td>0</td>\n",
       "      <td>[{\"tool_name\": \"get_product_details\", \"tool_in...</td>\n",
       "      <td>The response fully addresses both aspects of t...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The response is mostly fluent. It clearly answ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>The response is completely coherent, with a se...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The response provides product details and pric...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The response is exceptionally clear, coherent,...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The response is somewhat brief as it provides ...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Get details for usb charger</td>\n",
       "      <td>A super fast and light usb charger</td>\n",
       "      <td>2.439887</td>\n",
       "      <td>0</td>\n",
       "      <td>[{\"tool_name\": \"get_product_details\", \"tool_in...</td>\n",
       "      <td>The prompt asked for 'details' about a USB cha...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>The response is a fragment, not a complete sen...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>The response is highly illogical and lacks any...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The response is a simple description of a USB ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The response is fluent and coherent but fails ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>The response is excessively brief and complete...</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Get product details and price for shoes</td>\n",
       "      <td>High-performance running shoes designed for co...</td>\n",
       "      <td>2.56927</td>\n",
       "      <td>0</td>\n",
       "      <td>[{\"tool_name\": \"get_product_details\", \"tool_in...</td>\n",
       "      <td>The response successfully provides 'product de...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The response has a minor grammatical error as ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>The response seamlessly provides product detai...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The response is safe as it contains no hate sp...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The response is exceptionally clear, coherent,...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The response is slightly brief as it provides ...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Get product details for speaker?</td>\n",
       "      <td>A voice-controlled smart speaker that plays mu...</td>\n",
       "      <td>1.979517</td>\n",
       "      <td>0</td>\n",
       "      <td>[{\"tool_name\": \"get_product_details\", \"tool_in...</td>\n",
       "      <td>The response provides a general description of...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>The response is a concise and perfectly phrase...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The response is a single, perfectly formed sen...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The response is a neutral description of a sma...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The response is exceptionally clear, coherent,...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The response is slightly brief for a general r...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         prompt  \\\n",
       "0                      Get price for smartphone   \n",
       "1  Get product details and price for headphones   \n",
       "2                   Get details for usb charger   \n",
       "3       Get product details and price for shoes   \n",
       "4              Get product details for speaker?   \n",
       "\n",
       "                                            response latency_in_seconds  \\\n",
       "0                   The price for smartphone is 500.           2.279498   \n",
       "1  Here are the details for headphones: Wireless ...           2.561855   \n",
       "2                 A super fast and light usb charger           2.439887   \n",
       "3  High-performance running shoes designed for co...            2.56927   \n",
       "4  A voice-controlled smart speaker that plays mu...           1.979517   \n",
       "\n",
       "  failure                               predicted_trajectory  \\\n",
       "0       0  [{\"tool_name\": \"get_product_price\", \"tool_inpu...   \n",
       "1       0  [{\"tool_name\": \"get_product_details\", \"tool_in...   \n",
       "2       0  [{\"tool_name\": \"get_product_details\", \"tool_in...   \n",
       "3       0  [{\"tool_name\": \"get_product_details\", \"tool_in...   \n",
       "4       0  [{\"tool_name\": \"get_product_details\", \"tool_in...   \n",
       "\n",
       "                   instruction_following/explanation  \\\n",
       "0  The response directly addresses the instructio...   \n",
       "1  The response fully addresses both aspects of t...   \n",
       "2  The prompt asked for 'details' about a USB cha...   \n",
       "3  The response successfully provides 'product de...   \n",
       "4  The response provides a general description of...   \n",
       "\n",
       "   instruction_following/score  \\\n",
       "0                          5.0   \n",
       "1                          5.0   \n",
       "2                          2.0   \n",
       "3                          5.0   \n",
       "4                          3.0   \n",
       "\n",
       "                                 fluency/explanation  fluency/score  \\\n",
       "0  The response has a minor grammatical error due...            4.0   \n",
       "1  The response is mostly fluent. It clearly answ...            4.0   \n",
       "2  The response is a fragment, not a complete sen...            3.0   \n",
       "3  The response has a minor grammatical error as ...            4.0   \n",
       "4  The response is a concise and perfectly phrase...            5.0   \n",
       "\n",
       "                               coherence/explanation  coherence/score  \\\n",
       "0  The response is a single, clear, and direct st...              5.0   \n",
       "1  The response is completely coherent, with a se...              5.0   \n",
       "2  The response is highly illogical and lacks any...              1.0   \n",
       "3  The response seamlessly provides product detai...              5.0   \n",
       "4  The response is a single, perfectly formed sen...              5.0   \n",
       "\n",
       "                                  safety/explanation  safety/score  \\\n",
       "0  The response is safe as it contains no hate sp...           1.0   \n",
       "1  The response provides product details and pric...           1.0   \n",
       "2  The response is a simple description of a USB ...           1.0   \n",
       "3  The response is safe as it contains no hate sp...           1.0   \n",
       "4  The response is a neutral description of a sma...           1.0   \n",
       "\n",
       "                            text_quality/explanation  text_quality/score  \\\n",
       "0  The response is coherent, fluent, and perfectl...                 4.0   \n",
       "1  The response is exceptionally clear, coherent,...                 5.0   \n",
       "2  The response is fluent and coherent but fails ...                 2.0   \n",
       "3  The response is exceptionally clear, coherent,...                 5.0   \n",
       "4  The response is exceptionally clear, coherent,...                 5.0   \n",
       "\n",
       "                               verbosity/explanation  verbosity/score  \n",
       "0  The response is excessively brief and lacks cr...             -2.0  \n",
       "1  The response is somewhat brief as it provides ...             -1.0  \n",
       "2  The response is excessively brief and complete...             -2.0  \n",
       "3  The response is slightly brief as it provides ...             -1.0  \n",
       "4  The response is slightly brief for a general r...             -1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EXPERIMENT_RUN = f\"rubric-metric-eval-{get_id()}\"\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "\n",
    "eval_prompt_sample_dataset = pd.DataFrame(\n",
    "    {\"prompt\": [\n",
    "        \"Get price for smartphone\",\n",
    "        \"Get product details and price for headphones\",\n",
    "        \"Get details for usb charger\",\n",
    "        \"Get product details and price for shoes\",\n",
    "        \"Get product details for speaker?\",\n",
    "    ]\n",
    "    }\n",
    ")\n",
    "\n",
    "# Model-Based Metrics (uses judge LLM to evaluate response quality)\n",
    "response_quality_metrics = [\n",
    "    \"instruction_following\",  # Does the response answer the question?\n",
    "    \"fluency\",                # Is the response well-written?\n",
    "    \"coherence\",              # Is the response logically structured?\n",
    "    \"safety\",                 # Is the response safe/appropriate?\n",
    "    \"text_quality\",           # Overall text quality\n",
    "    \"verbosity\",              # Is response too long/short?\n",
    "]\n",
    "\n",
    "response_quality_result = EvalTask(\n",
    "    dataset = eval_prompt_sample_dataset,\n",
    "    metrics = response_quality_metrics,\n",
    "    experiment = EXPERIMENT_NAME,\n",
    "    output_uri_prefix = BUCKET_URI + \"/rubric-metric-eval\"\n",
    ").evaluate(\n",
    "    runnable = agent_parsed_outcome_sync, # Generate responses on-the-fly (not BYOD mode)\n",
    "    experiment_run_name=EXPERIMENT_RUN\n",
    ")\n",
    "\n",
    "display_eval_report(response_quality_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4bENwFcd6prX"
   },
   "source": [
    "# Eval #2\n",
    "## Define a Custom Metric\n",
    "\n",
    "According to the [documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/models/determine-eval#model-based-metrics), you can define a prompt template for evaluating whether an AI agent's response follows logically from its actions by setting up criteria and a rating system for this evaluation.\n",
    "\n",
    "Define a `criteria` to set the evaluation guidelines and a `pointwise_rating_rubric` to provide a scoring system (1 or 0). Then use a `PointwiseMetricPromptTemplate` to create the template using these components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = {\n",
    "    \"Request Completeness\": (\n",
    "        \"You are a Quality Assurance Analyst. Your task is to evaluate if an AI agent's response \"\n",
    "        \"completely fulfills a user's shopping query based on a single criterion: Request Completeness.\\n\\n\"\n",
    "        \"Assign a score based on whether the agent provided all the categories of information the user asked for.\\n\\n\"\n",
    "        \"Instructions:\\n\"\n",
    "        \"  - Read the user's question to identify all requested information types (e.g., 'price', 'details').\\n\"\n",
    "        \"  - Analyze the response to see which information types were provided.\\n\"\n",
    "        \"  - If all requested types are present, score '1'. Otherwise, score '0'.\\n\\n\"\n",
    "        \"For example, if the user asks for 'price and details,' the response must contain both a price and \"\n",
    "        \"some form of product details.\"\n",
    "    )\n",
    "}\n",
    "\n",
    "pointwise_rating_rubric = {\n",
    "    \"1\": \"The response provides all the types of information explicitly requested in the question.\",\n",
    "    \"0\": \"The response is missing at least one type of information explicitly requested in the question.\",\n",
    "}\n",
    "\n",
    "response_completeness_prompt_template = PointwiseMetricPromptTemplate(\n",
    "    criteria=criteria,\n",
    "    rating_rubric=pointwise_rating_rubric,\n",
    "    input_variables=[\"prompt\"],\n",
    ")\n",
    "\n",
    "response_completeness_metric = PointwiseMetric(\n",
    "    metric=\"response_completeness\",\n",
    "    metric_prompt_template=response_completeness_prompt_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_dkb4gSn7Ywv"
   },
   "outputs": [],
   "source": [
    "#### Run an evaluation task\n",
    "\n",
    "EXPERIMENT_RUN = f\"response-over-tools-{get_id()}\"\n",
    "\n",
    "response_eval_tool_result = EvalTask(\n",
    "    dataset = eval_sample_dataset,\n",
    "    metrics = [response_completeness_metric],\n",
    "    experiment=EXPERIMENT_NAME,\n",
    "    output_uri_prefix=BUCKET_URI + \"/reasoning-metric-eval\",\n",
    ").evaluate(\n",
    "    runnable = agent_parsed_outcome_sync,\n",
    "    experiment_run_name=EXPERIMENT_RUN\n",
    ")\n",
    "\n",
    "display_eval_report(response_eval_tool_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval #3\n",
    "## Custom Function-Based Metrics\n",
    "\n",
    "Python functions for deterministic agent evaluation without LLM (Judges) calls.\n",
    "\n",
    "### Properties\n",
    "- Deterministic output\n",
    "- Explicit business logic encoding\n",
    "- Standard Python debugging\n",
    "\n",
    "### Custom metric functions must:\n",
    "\n",
    "1. Accept evaluation instance: `def metric_fn(instance: dict) -> dict`\n",
    "2. Return dictionary with metric name and numeric score\n",
    "3. Be wrapped in `CustomMetric` for `EvalTask` integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell defines custom evaluation functions/metrics for agent behavior validation.\n",
    "\n",
    "def tool_count_metric(instance: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Counts the number of tools called in the predicted trajectory.\n",
    "    \n",
    "    Args:\n",
    "        instance: Dictionary containing 'predicted_trajectory' key\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with metric name and score\n",
    "    \"\"\"\n",
    "    trajectory = instance.get(\"predicted_trajectory\", \"\")\n",
    "    \n",
    "    # Handle both string (JSON) and list formats\n",
    "    if isinstance(trajectory, str):\n",
    "        try:\n",
    "            trajectory = json.loads(trajectory)\n",
    "        except json.JSONDecodeError:\n",
    "            trajectory = []\n",
    "    \n",
    "    tool_count = len(trajectory) if isinstance(trajectory, list) else 0\n",
    "    \n",
    "    return {\n",
    "        \"tool_count\": tool_count\n",
    "    }\n",
    "\n",
    "\n",
    "def tool_efficiency_metric(instance: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Measures if the agent uses the minimum necessary tools.\n",
    "    Score of 1 if tool count <= 2, otherwise 0.\n",
    "    \n",
    "    Args:\n",
    "        instance: Dictionary containing 'predicted_trajectory' key\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with metric name and score (0 or 1)\n",
    "    \"\"\"\n",
    "    trajectory = instance.get(\"predicted_trajectory\", \"\")\n",
    "    \n",
    "    if isinstance(trajectory, str):\n",
    "        try:\n",
    "            trajectory = json.loads(trajectory)\n",
    "        except json.JSONDecodeError:\n",
    "            trajectory = []\n",
    "    \n",
    "    tool_count = len(trajectory) if isinstance(trajectory, list) else 0\n",
    "    \n",
    "    # Efficient if using 2 or fewer tools\n",
    "    is_efficient = 1 if tool_count <= 2 else 0\n",
    "    \n",
    "    return {\n",
    "        \"tool_efficiency\": is_efficient\n",
    "    }\n",
    "def response_length_metric(instance: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Measures the length of the response in characters.\n",
    "    \n",
    "    Args:\n",
    "        instance: Dictionary containing 'response' key\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with metric name and score\n",
    "    \"\"\"\n",
    "    response = instance.get(\"response\", \"\")\n",
    "    \n",
    "    # Handle JSON-encoded strings\n",
    "    if isinstance(response, str) and response.startswith('\"'):\n",
    "        try:\n",
    "            response = json.loads(response)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    \n",
    "    response_length = len(str(response))\n",
    "    \n",
    "    return {\n",
    "        \"response_length\": response_length\n",
    "    }\n",
    "\n",
    "\n",
    "def response_conciseness_metric(instance: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Checks if response is concise (under 200 characters).\n",
    "    Score of 1 if concise, 0 otherwise.\n",
    "    \n",
    "    Args:\n",
    "        instance: Dictionary containing 'response' key\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with metric name and score (0 or 1)\n",
    "    \"\"\"\n",
    "    response = instance.get(\"response\", \"\")\n",
    "    \n",
    "    if isinstance(response, str) and response.startswith('\"'):\n",
    "        try:\n",
    "            response = json.loads(response)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    \n",
    "    is_concise = 1 if len(str(response)) <= 200 else 0\n",
    "    \n",
    "    return {\n",
    "        \"response_conciseness\": is_concise\n",
    "    }\n",
    "\n",
    "\n",
    "def numeric_response_metric(instance: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Validates if response contains numeric values (for price queries).\n",
    "    Score of 1 if numeric, 0 otherwise.\n",
    "    \n",
    "    Args:\n",
    "        instance: Dictionary containing 'response' key\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with metric name and score (0 or 1)\n",
    "    \"\"\"\n",
    "    response = instance.get(\"response\", \"\")\n",
    "    \n",
    "    if isinstance(response, str) and response.startswith('\"'):\n",
    "        try:\n",
    "            response = json.loads(response)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    \n",
    "    # Check if response contains digits or is a number\n",
    "    response_str = str(response).strip()\n",
    "    has_number = any(char.isdigit() for char in response_str)\n",
    "    \n",
    "    # Or check if it can be converted to a number\n",
    "    try:\n",
    "        float(response_str)\n",
    "        is_numeric = 1\n",
    "    except ValueError:\n",
    "        is_numeric = 1 if has_number else 0\n",
    "    \n",
    "    return {\n",
    "        \"numeric_response\": is_numeric\n",
    "    }\n",
    "def valid_product_metric(instance: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Validates that the agent only queries valid products from the catalog.\n",
    "    \n",
    "    Args:\n",
    "        instance: Dictionary containing 'predicted_trajectory' key\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with metric name and score (0 or 1)\n",
    "    \"\"\"\n",
    "    VALID_PRODUCTS = {\"smartphone\", \"usb charger\", \"shoes\", \"headphones\", \"speaker\"}\n",
    "    \n",
    "    trajectory = instance.get(\"predicted_trajectory\", \"\")\n",
    "    \n",
    "    if isinstance(trajectory, str):\n",
    "        try:\n",
    "            trajectory = json.loads(trajectory)\n",
    "        except json.JSONDecodeError:\n",
    "            trajectory = []\n",
    "    \n",
    "    # Check all tool inputs for valid product names\n",
    "    all_valid = True\n",
    "    if isinstance(trajectory, list):\n",
    "        for tool_call in trajectory:\n",
    "            if isinstance(tool_call, dict):\n",
    "                tool_input = tool_call.get(\"tool_input\", {})\n",
    "                product_name = tool_input.get(\"product_name\", \"\")\n",
    "                if product_name and product_name not in VALID_PRODUCTS:\n",
    "                    all_valid = False\n",
    "                    break\n",
    "    \n",
    "    return {\n",
    "        \"valid_product\": 1 if all_valid else 0\n",
    "    }\n",
    "\n",
    "\n",
    "def correct_tool_selection_metric(instance: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Validates that the agent uses the correct tool based on the prompt.\n",
    "    - Price queries should use 'get_product_price'\n",
    "    - Details queries should use 'get_product_details'\n",
    "    \n",
    "    Args:\n",
    "        instance: Dictionary containing 'prompt' and 'predicted_trajectory' keys\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with metric name and score (0 or 1)\n",
    "    \"\"\"\n",
    "    prompt = instance.get(\"prompt\", \"\").lower()\n",
    "    trajectory = instance.get(\"predicted_trajectory\", \"\")\n",
    "    \n",
    "    if isinstance(trajectory, str):\n",
    "        try:\n",
    "            trajectory = json.loads(trajectory)\n",
    "        except json.JSONDecodeError:\n",
    "            return {\"correct_tool_selection\": 0}\n",
    "    \n",
    "    if not isinstance(trajectory, list) or len(trajectory) == 0:\n",
    "        return {\"correct_tool_selection\": 0}\n",
    "    \n",
    "    # Extract tool names used\n",
    "    tools_used = [call.get(\"tool_name\", \"\") for call in trajectory if isinstance(call, dict)]\n",
    "    \n",
    "    correct = False\n",
    "    \n",
    "    # Check if correct tools were used based on prompt\n",
    "    if \"price\" in prompt and \"get_product_price\" in tools_used:\n",
    "        correct = True\n",
    "    elif \"details\" in prompt and \"price\" not in prompt and \"get_product_details\" in tools_used:\n",
    "        correct = True\n",
    "    elif \"details\" in prompt and \"price\" in prompt:\n",
    "        # Both tools should be used\n",
    "        correct = \"get_product_details\" in tools_used and \"get_product_price\" in tools_used\n",
    "    \n",
    "    return {\n",
    "        \"correct_tool_selection\": 1 if correct else 0\n",
    "    }\n",
    "\n",
    "\n",
    "def price_range_validation_metric(instance: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Validates that price responses are within expected range ($1-$1000).\n",
    "    \n",
    "    Args:\n",
    "        instance: Dictionary containing 'response' key\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with metric name and score (0 or 1)\n",
    "    \"\"\"\n",
    "    response = instance.get(\"response\", \"\")\n",
    "    \n",
    "    if isinstance(response, str) and response.startswith('\"'):\n",
    "        try:\n",
    "            response = json.loads(response)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    \n",
    "    response_str = str(response).strip()\n",
    "    \n",
    "    # Try to extract numeric value\n",
    "    try:\n",
    "        # Remove currency symbols and convert to float\n",
    "        price = float(response_str.replace(\"$\", \"\").replace(\",\", \"\"))\n",
    "        valid = 1 if 1 <= price <= 1000 else 0\n",
    "    except ValueError:\n",
    "        # If not a price response, consider it valid (not applicable)\n",
    "        valid = 1\n",
    "    \n",
    "    return {\n",
    "        \"price_range_valid\": valid\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define CustomMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap all custom metrics using CustomMetric class\n",
    "custom_metrics = [\n",
    "    # Tool usage metrics\n",
    "    CustomMetric(name=\"tool_count\", metric_function=tool_count_metric),\n",
    "    CustomMetric(name=\"tool_efficiency\", metric_function=tool_efficiency_metric),\n",
    "    \n",
    "    # Response quality metrics\n",
    "    CustomMetric(name=\"response_length\", metric_function=response_length_metric),\n",
    "    CustomMetric(name=\"response_conciseness\", metric_function=response_conciseness_metric),\n",
    "    CustomMetric(name=\"numeric_response\", metric_function=numeric_response_metric),\n",
    "    \n",
    "    # Business logic metrics\n",
    "    CustomMetric(name=\"valid_product\", metric_function=valid_product_metric),\n",
    "    CustomMetric(name=\"correct_tool_selection\", metric_function=correct_tool_selection_metric),\n",
    "    CustomMetric(name=\"price_range_valid\", metric_function=price_range_validation_metric),\n",
    "]\n",
    "\n",
    "print(f\"✓ Created {len(custom_metrics)} custom metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation with custom metrics\n",
    "\n",
    "# Generate a unique experiment run identifier\n",
    "EXPERIMENT_RUN = f\"custom-metrics-eval-{get_id()}\"\n",
    "\n",
    "# Execute the evaluation task with custom deterministic metrics\n",
    "custom_metrics_result = EvalTask(\n",
    "    dataset=eval_prompt_sample_dataset,  # Dataset with only prompts (agent generates responses during eval)\n",
    "    metrics=custom_metrics,  # List of 8 custom function-based metrics defined above\n",
    "    experiment=EXPERIMENT_NAME,  # Vertex AI experiment name for tracking across runs\n",
    "    output_uri_prefix=BUCKET_URI + \"/custom-metrics-eval\",  # GCS path to persist results\n",
    ").evaluate(\n",
    "    runnable=agent_parsed_outcome_sync,  # Wrapper function that invokes the agent for each prompt\n",
    "    experiment_run_name=EXPERIMENT_RUN  # Unique name for this specific evaluation run\n",
    ")\n",
    "\n",
    "# Display summary statistics and row-level metrics in formatted tables\n",
    "display_eval_report(custom_metrics_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed metrics for each row\n",
    "display(Markdown(\"### Detailed Results by Instance\"))\n",
    "display(custom_metrics_result.metrics_table)\n",
    "\n",
    "# Show specific metrics of interest\n",
    "display(Markdown(\"### Key Custom Metrics Summary\"))\n",
    "key_metrics = [\n",
    "    \"tool_efficiency/mean\",\n",
    "    \"correct_tool_selection/mean\", \n",
    "    \"valid_product/mean\",\n",
    "    \"response_conciseness/mean\"\n",
    "]\n",
    "\n",
    "summary_df = pd.DataFrame([\n",
    "    {\"Metric\": metric.replace(\"/mean\", \"\").replace(\"_\", \" \").title(), \n",
    "     \"Score\": custom_metrics_result.summary_metrics.get(metric, \"N/A\")}\n",
    "    for metric in key_metrics\n",
    "])\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIppkS2jq_Dn"
   },
   "source": [
    "## Cleaning up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ox2I3UfRlTOd"
   },
   "outputs": [],
   "source": [
    "### Optional: Delete Experiment Artifacts\n",
    "delete_experiment = True\n",
    "\n",
    "if delete_experiment:\n",
    "    try:\n",
    "        experiment = aiplatform.Experiment(EXPERIMENT_NAME)\n",
    "        experiment.delete(delete_backing_tensorboard_runs=True)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank you"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "evaluating_adk_agent.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
