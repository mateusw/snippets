{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2025 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "# ADK Agent Evaluation with Vertex AI Gen AI Evaluation Service\n",
    "\n",
    "> **Note**: This notebook is for reference and educational purposes only. Not intended for production use.  \n",
    "> **Questions?** mateuswagner@google.com\n",
    "\n",
    "This notebook demonstrates how to build and systematically evaluate AI agents using Google's Agent Development Kit (ADK) and Vertex AI Gen AI Evaluation Service.\n",
    "\n",
    "## What You'll Build\n",
    "\n",
    "A product research agent that retrieves product details and prices, evaluated using three complementary approaches to measure quality, tool usage, and business logic compliance.\n",
    "\n",
    "**Tech Stack**: ADK + Gemini + Vertex AI Gen AI Evaluation Service\n",
    "\n",
    "## Evaluation Approaches\n",
    "\n",
    "### 1. Rubric-Based Metrics (LLM-as-Judge)\n",
    "\n",
    "Uses a separate LLM judge to evaluate response quality with pre-built metrics:\n",
    "- `instruction_following` - Answers the question asked\n",
    "- `fluency` - Grammatically correct and natural\n",
    "- `coherence` - Logically consistent and clear\n",
    "- `safety` - Free from harmful content\n",
    "- `text_quality` - Overall quality assessment\n",
    "- `verbosity` - Appropriate response length\n",
    "\n",
    "**When to use**: Automated quality assurance for customer-facing agent responses\n",
    "\n",
    "### 2. **Custom** Metrics (LLM-as-Judge)\n",
    "\n",
    "Evaluates domain-specific criteria using custom prompt templates:\n",
    "\n",
    "**When to use**: Business-specific quality validation beyond generic metrics\n",
    "\n",
    "### 3. Custom Function Metrics (Deterministic)\n",
    "\n",
    "Custom Python functions that validate agent behavior without Judges calls:\n",
    "\n",
    "**Tool Usage Validation**:\n",
    "- `tool_count` - Number of tools called\n",
    "- `tool_efficiency` - Uses minimum necessary tools\n",
    "\n",
    "**Response Validation**:\n",
    "- `response_length` - Character count\n",
    "- `response_conciseness` - Appropriately brief\n",
    "- `numeric_response` - Contains numbers when expected\n",
    "\n",
    "**Business Logic Validation**:\n",
    "- `valid_product` - Queries only catalog products\n",
    "- `correct_tool_selection` - Uses appropriate tool for query type\n",
    "- `price_range_validation` - Price within expected bounds\n",
    "\n",
    "**When to use**: Fast, deterministic validation of agent logic and compliance\n",
    "\n",
    "## Implementation Workflow\n",
    "\n",
    "**Setup**\n",
    "\n",
    "1. Install dependencies and configure GCP project\n",
    "2. Initialize Vertex AI with experiment tracking\n",
    "3. Define helper functions for parsing and display\n",
    "\n",
    "**Agent Development**\n",
    "\n",
    "4. Create custom tools for product research\n",
    "5. Configure Gemini model and build ADK agent\n",
    "6. Create evaluation dataset with expected behaviors\n",
    "\n",
    "**Evaluation**\n",
    "\n",
    "7. Run rubric-based evaluation for response quality\n",
    "8. Run custom pointwise evaluation for completeness\n",
    "9. Run function-based evaluation for logic validation\n",
    "10. Compare metrics across evaluation approaches\n",
    "11. Review detailed results and summary statistics\n",
    "\n",
    "## Evaluation Dataset Structure\n",
    "\n",
    "The dataset serves as ground truth for measuring agent performance.\n",
    "\n",
    "This enables:\n",
    "- Trajectory validation against expected tool usage\n",
    "- Regression testing for agent updates\n",
    "- Systematic coverage of edge cases\n",
    "- Objective baselines for automated metrics\n",
    "\n",
    "## Evaluation Modes\n",
    "\n",
    "**On-the-fly generation** (used in this notebook): Provide only prompts, agent generates responses during evaluation\n",
    "\n",
    "**BYOD (Bring Your Own Data)**: Provide prompts, responses, and trajectories for faster evaluation of pre-generated results\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "**LLM-as-Judge**: Uses a separate model to evaluate response quality based on rubrics and criteria. Provides nuanced assessment but has API costs.\n",
    "\n",
    "**Deterministic Metrics**: Python functions that validate specific behaviors. Fast, free, and reproducible but limited to predefined rules.\n",
    "\n",
    "**Complementary Strategy**: Combine both approaches for comprehensive agent evaluation - quality assessment via LLM judges and logic validation via deterministic functions.\n",
    "\n",
    "## Outputs\n",
    "\n",
    "All evaluation results are tracked in Vertex AI Experiments and persisted to Cloud Storage for reproducibility and comparison across runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61RBz8LLbxCR"
   },
   "source": [
    "## Get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "No17Cw5hgx12"
   },
   "outputs": [],
   "source": [
    "## Installation\n",
    "# Python 3.10+ recommended\n",
    "# Virtual environment (venv) recommended for isolation\n",
    "# Google Cloud SDK initialized: `gcloud init`\n",
    "\n",
    "%pip install --upgrade --quiet 'google-adk' nbformat 'google-cloud-aiplatform[evaluation]'\n",
    "\n",
    "# Restart your Jupyter kernel !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5303c05f7aa6"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6fc324893334"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/snippets/.venv/lib/python3.12/site-packages/google/cloud/aiplatform/models.py:52: FutureWarning: Support for google-cloud-storage < 3.0.0 will be removed in a future version of google-cloud-aiplatform. Please upgrade to google-cloud-storage >= 3.0.0.\n",
      "  from google.cloud.aiplatform.utils import gcs_utils\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "import warnings\n",
    "from typing import Any\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "from IPython.display import HTML, Markdown, display\n",
    "\n",
    "# Google Cloud imports\n",
    "import vertexai\n",
    "from google.cloud import aiplatform\n",
    "from google.genai import types\n",
    "from google.cloud import storage\n",
    "from google.api_core import exceptions\n",
    "\n",
    "# Google ADK imports\n",
    "from google.adk.agents import Agent\n",
    "from google.adk.events import Event\n",
    "from google.adk.runners import Runner\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "\n",
    "# Vertex AI Evaluation imports\n",
    "from vertexai.preview.evaluation import EvalTask\n",
    "from vertexai.preview.evaluation.metrics import (\n",
    "    CustomMetric,\n",
    "    PointwiseMetric,\n",
    "    PointwiseMetricPromptTemplate,\n",
    "    TrajectorySingleToolUse,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONFIGURATION SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nqwi-5ufWp_B"
   },
   "outputs": [],
   "source": [
    "# Enable Vertex AI for Google Gen AI SDK\n",
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"true\"\n",
    "\n",
    "# Google Cloud Project Configuration\n",
    "# !!! CHANGE THIS: Set your Google Cloud project ID !!!\n",
    "PROJECT_ID = \"matt-demos\" # CHANGE IT!\n",
    "\n",
    "# Fallback to environment variable if not set\n",
    "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
    "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
    "\n",
    "# Default region for Vertex AI resources\n",
    "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")\n",
    "\n",
    "# Cloud Storage Configuration\n",
    "# !!! CHANGE THIS: Set your Cloud Storage bucket name !!!\n",
    "BUCKET_NAME = \"53642dcf-cdb9-4f6c-a3bb-cf6595602893\"\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
    "os.environ[\"GOOGLE_CLOUD_LOCATION\"] = LOCATION\n",
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"True\"\n",
    "\n",
    "# Vertex AI Experiments name for tracking evaluation runs\n",
    "EXPERIMENT_NAME = \"evaluate-adk-agent\"\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Initialize Vertex AI with experiment tracking\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION, experiment=EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Create GCS bucket (skip if already exists)\n",
    "\n",
    "def create_bucket_if_not_exists(bucket_name: str, location: str) -> None:\n",
    "    \"\"\"Create a GCS bucket if it doesn't already exist.\"\"\"\n",
    "    storage_client = storage.Client(project=PROJECT_ID)\n",
    "    \n",
    "    try:\n",
    "        bucket = storage_client.lookup_bucket(bucket_name)\n",
    "        \n",
    "        if bucket is not None:\n",
    "            print(f\"✓ Bucket '{bucket_name}' already exists - skipping creation\")\n",
    "            print(f\"  Location: {bucket.location} | Created: {bucket.time_created}\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Creating bucket '{bucket_name}' in {location}...\")\n",
    "        bucket = storage_client.create_bucket(bucket_name, location=location)\n",
    "        print(f\"✓ Successfully created bucket '{bucket_name}' at gs://{bucket_name}\")\n",
    "        \n",
    "    except exceptions.Forbidden as e:\n",
    "        print(f\"✗ Permission denied. Ensure you have 'storage.buckets.create' permission\")\n",
    "        raise\n",
    "    except exceptions.Conflict as e:\n",
    "        print(f\"✗ Bucket name already taken globally. Try a different name.\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {e}\")\n",
    "        raise\n",
    "\n",
    "try:\n",
    "    create_bucket_if_not_exists(BUCKET_NAME, LOCATION)\n",
    "except Exception:\n",
    "    print(\"\\nBucket creation failed. Continue if bucket exists or create manually\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVnBDX54gz7j"
   },
   "source": [
    "## Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uSgWjMD_g1_v"
   },
   "outputs": [],
   "source": [
    "def get_id(length: int = 8) -> str:\n",
    "    \"\"\"Generate a uuid of a specified length (default=8).\"\"\"\n",
    "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "\n",
    "def parse_adk_output_to_dictionary(events: list[Event], *, as_json: bool = False):\n",
    "    \"\"\"\n",
    "    Parse ADK event output into a structured dictionary format,\n",
    "    with the predicted trajectory dumped as a JSON string.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    final_response = \"\"\n",
    "    trajectory = []\n",
    "\n",
    "    for event in events:\n",
    "        if not getattr(event, \"content\", None) or not getattr(event.content, \"parts\", None):\n",
    "            continue\n",
    "        for part in event.content.parts:\n",
    "            if getattr(part, \"function_call\", None):\n",
    "                info = {\n",
    "                    \"tool_name\": part.function_call.name,\n",
    "                    \"tool_input\": dict(part.function_call.args),\n",
    "                }\n",
    "                if info not in trajectory:\n",
    "                    trajectory.append(info)\n",
    "            if event.content.role == \"model\" and getattr(part, \"text\", None):\n",
    "                final_response = part.text.strip()\n",
    "\n",
    "    if as_json:\n",
    "        trajectory_out = json.dumps(trajectory)\n",
    "    else:\n",
    "        trajectory_out = trajectory\n",
    "\n",
    "    return {\"response\": final_response, \"predicted_trajectory\": trajectory_out}\n",
    "\n",
    "\n",
    "def format_output_as_markdown(output: dict) -> str:\n",
    "    \"\"\"Convert the output dictionary to a formatted markdown string.\"\"\"\n",
    "    markdown = \"### AI Response\\n\" + output[\"response\"] + \"\\n\\n\"\n",
    "    if output[\"predicted_trajectory\"]:\n",
    "        markdown += \"### Function Calls\\n\"\n",
    "        for call in output[\"predicted_trajectory\"]:\n",
    "            markdown += f\"- **Function**: `{call['tool_name']}`\\n\"\n",
    "            markdown += \"  - **Arguments**\\n\"\n",
    "            for key, value in call[\"tool_input\"].items():\n",
    "                markdown += f\"    - `{key}`: `{value}`\\n\"\n",
    "    return markdown\n",
    "\n",
    "\n",
    "def display_eval_report(eval_result: pd.DataFrame) -> None:\n",
    "    \"\"\"Display the evaluation results.\"\"\"\n",
    "    display(Markdown(\"### Summary Metrics\"))\n",
    "    display(\n",
    "        pd.DataFrame(\n",
    "            eval_result.summary_metrics.items(), columns=[\"metric\", \"value\"]\n",
    "        )\n",
    "    )\n",
    "    if getattr(eval_result, \"metrics_table\", None) is not None:\n",
    "        display(Markdown(\"### Row‑wise Metrics\"))\n",
    "        display(eval_result.metrics_table.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDaa2Mtsifmq"
   },
   "source": [
    "## Build ADK agent\n",
    "\n",
    "Build your application using ADK, including the Gemini model and custom tools that you define.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHwShhpOitKp"
   },
   "source": [
    "### Set agent tools\n",
    "\n",
    "To start, set the tools that a customer support agent needs to do their job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gA2ZKvfeislw"
   },
   "outputs": [],
   "source": [
    "def get_product_details(product_name: str):\n",
    "    \"\"\"Gathers basic details about a product.\"\"\"\n",
    "    details = {\n",
    "        \"smartphone\": \"A cutting-edge smartphone with advanced camera features and lightning-fast processing.\",\n",
    "        \"usb charger\": \"A super fast and light usb charger\",\n",
    "        \"shoes\": \"High-performance running shoes designed for comfort, support, and speed.\",\n",
    "        \"headphones\": \"Wireless headphones with advanced noise cancellation technology for immersive audio.\",\n",
    "        \"speaker\": \"A voice-controlled smart speaker that plays music, sets alarms, and controls smart home devices.\",\n",
    "    }\n",
    "    return details.get(product_name, \"Product details not found.\")\n",
    "\n",
    "\n",
    "def get_product_price(product_name: str):\n",
    "    \"\"\"Gathers price about a product.\"\"\"\n",
    "    details = {\n",
    "        \"smartphone\": 500,\n",
    "        \"usb charger\": 10,\n",
    "        \"shoes\": 100,\n",
    "        \"headphones\": 50,\n",
    "        \"speaker\": 80,\n",
    "    }\n",
    "    return details.get(product_name, \"Product price not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4mk5XPui4Y1"
   },
   "source": [
    "### Set Agent the model\n",
    "\n",
    "Configure the Gemini model for your ADK agent. This notebook uses **`gemini-2.5-flash`** for fast, cost-effective function calling.\n",
    "\n",
    "**Model Selection Guidelines:**\n",
    "- **`gemini-2.5-flash`**: Fast responses, low cost, ideal for production agents with straightforward tool usage\n",
    "- **`gemini-2.5-pro`**: Higher reasoning capability, better for complex multi-step workflows and ambiguous queries\n",
    "\n",
    "See the [Gemini model documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models) for detailed performance benchmarks and pricing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BaYeo6K2i-w1"
   },
   "outputs": [],
   "source": [
    "model = \"gemini-2.5-flash\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNlAY9cojEWz"
   },
   "source": [
    "### Assemble the ADK Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gD5OB44g4sc3"
   },
   "outputs": [],
   "source": [
    "async def agent_parsed_outcome(query):\n",
    "   # Session identifiers for tracking agent interactions\n",
    "   app_name = \"product_research_app\"\n",
    "   user_id = \"user_one\"\n",
    "   session_id = \"session_one\"\n",
    "   \n",
    "   # Create agent with dynamic instruction based on query\n",
    "   product_research_agent = Agent(\n",
    "       name=\"ProductResearchAgent\",\n",
    "       model=model,\n",
    "       description=\"An agent that performs product research.\",\n",
    "       instruction=f\"\"\"\n",
    "       Analyze this user request: '{query}'.\n",
    "       If the request is about price, use get_product_price tool.\n",
    "       Otherwise, use get_product_details tool to get product information.\n",
    "       \"\"\",\n",
    "       tools=[get_product_details, get_product_price],\n",
    "   )\n",
    "\n",
    "   # Initialize in-memory session storage\n",
    "   session_service = InMemorySessionService()\n",
    "   await session_service.create_session(\n",
    "       app_name=app_name, user_id=user_id, session_id=session_id\n",
    "   )\n",
    "\n",
    "   # Create runner to execute agent with session management\n",
    "   runner = Runner(\n",
    "       agent=product_research_agent, app_name=app_name, session_service=session_service\n",
    "   )\n",
    "\n",
    "   # Format query as user message and run agent asynchronously\n",
    "   content = types.Content(role=\"user\", parts=[types.Part(text=query)])\n",
    "   events = [event async for event in runner.run_async(user_id=user_id, session_id=session_id, new_message=content)]\n",
    "   \n",
    "   # Parse events into dictionary with response and tool calls\n",
    "   return parse_adk_output_to_dictionary(events)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent Wrapper for Vertex AI Evaluation Service\n",
    "# This function will be passed to EvalTask.evaluate(runnable=agent_parsed_outcome_sync)\n",
    "# Vertex AI will call it for each prompt in the evaluation dataset, automatically generating\n",
    "# responses and trajectories on-the-fly for metrics computation.\n",
    "\n",
    "def agent_parsed_outcome_sync(prompt: str):\n",
    "    result = asyncio.run(agent_parsed_outcome(prompt))\n",
    "    result[\"predicted_trajectory\"] = json.dumps(result[\"predicted_trajectory\"])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lGb58OJkjUs9"
   },
   "outputs": [],
   "source": [
    "# Test the agent\n",
    "\n",
    "response = await agent_parsed_outcome(query=\"Get product details for shoes\")\n",
    "display(Markdown(format_output_as_markdown(response)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e43229f3ad4f"
   },
   "source": [
    "### Prepare Agent Evaluation Dataset\n",
    "\n",
    "The evaluation dataset serves as the **ground truth benchmark** for measuring agent performance. It contains:\n",
    "- **`prompt`**: Test queries that cover diverse agent scenarios (price lookups, detail requests, multi-step tasks)\n",
    "- **`predicted_trajectory`**: Expected tool call sequences that define correct agent behavior\n",
    "\n",
    "**Uses:**\n",
    "- **Trajectory Validation**: Compare actual vs expected tool usage to catch logic errors (wrong tools, missing steps, extra calls)\n",
    "- **Regression Testing**: Ensure agent improvements don't break existing functionality\n",
    "- **Coverage Analysis**: Systematically test edge cases and multi-tool workflows\n",
    "- **Baseline for Metrics**: Powers both automated metrics (tool selection, efficiency) and LLM-based judges (response quality)\n",
    "\n",
    "Without this dataset, you're evaluating in a vacuum with no objective standard for correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fFf8uTdUiDt3"
   },
   "outputs": [],
   "source": [
    "eval_data = {\n",
    "    \"prompt\": [\n",
    "        \"Get price for smartphone\",\n",
    "        \"Get product details and price for headphones\",\n",
    "        \"Get details for usb charger\",\n",
    "        \"Get product details and price for shoes\",\n",
    "        \"Get product details for speaker?\",\n",
    "    ],\n",
    "    \"predicted_trajectory\": [\n",
    "        [\n",
    "            {\n",
    "                \"tool_name\": \"get_product_price\",\n",
    "                \"tool_input\": {\"product_name\": \"smartphone\"},\n",
    "            }\n",
    "        ],\n",
    "        [\n",
    "            {\n",
    "                \"tool_name\": \"get_product_details\",\n",
    "                \"tool_input\": {\"product_name\": \"headphones\"},\n",
    "            },\n",
    "            {\n",
    "                \"tool_name\": \"get_product_price\",\n",
    "                \"tool_input\": {\"product_name\": \"headphones\"},\n",
    "            },\n",
    "        ],\n",
    "        [\n",
    "            {\n",
    "                \"tool_name\": \"get_product_details\",\n",
    "                \"tool_input\": {\"product_name\": \"usb charger\"},\n",
    "            }\n",
    "        ],\n",
    "        [\n",
    "            {\n",
    "                \"tool_name\": \"get_product_details\",\n",
    "                \"tool_input\": {\"product_name\": \"shoes\"},\n",
    "            },\n",
    "            {\"tool_name\": \"get_product_price\", \"tool_input\": {\"product_name\": \"shoes\"}},\n",
    "        ],\n",
    "        [\n",
    "            {\n",
    "                \"tool_name\": \"get_product_details\",\n",
    "                \"tool_input\": {\"product_name\": \"speaker\"},\n",
    "            }\n",
    "        ],\n",
    "    ],\n",
    "}\n",
    "\n",
    "eval_sample_dataset = pd.DataFrame(eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval #1\n",
    "## Rubric-Based Evaluation (Model-Based Metrics)\n",
    "\n",
    "Evaluates the **quality of agent text responses** using an LLM as a judge. Unlike trajectory metrics that validate tool usage, these metrics assess communication quality.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "A separate judge LLM (Gemini) evaluates each response by analyzing:\n",
    "- User prompt\n",
    "- Agent response\n",
    "- Metric-specific rubric/criteria\n",
    "\n",
    "### Metrics (6 model-based)\n",
    "\n",
    "| Metric | Evaluation Criteria | Scale |\n",
    "|--------|---------------------|-------|\n",
    "| `instruction_following` | Answers the question asked | 1-5 |\n",
    "| `fluency` | Grammatically correct and natural | 1-5 |\n",
    "| `coherence` | Logically consistent and clear | 1-5 |\n",
    "| `safety` | Free from harmful content | 1 (safe) / 0 (unsafe) |\n",
    "| `text_quality` | Overall quality | 1-5 |\n",
    "| `verbosity` | Appropriate response length | -2 to +2 (0 = optimal) |\n",
    "\n",
    "### Evaluation Modes\n",
    "\n",
    "1. **On-the-fly (used here)**: Provide only `prompt` column, agent generates responses during evaluation\n",
    "\n",
    "### Output\n",
    "\n",
    "- **GCS**: `gs://{BUCKET_URI}/rubric-metric-eval/`\n",
    "- **Tracking**: Logged to Vertex AI Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_RUN = f\"rubric-metric-eval-{get_id()}\"\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "\n",
    "eval_prompt_sample_dataset = pd.DataFrame(\n",
    "    {\"prompt\": [\n",
    "        \"Get price for smartphone\",\n",
    "        \"Get product details and price for headphones\",\n",
    "        \"Get details for usb charger\",\n",
    "        \"Get product details and price for shoes\",\n",
    "        \"Get product details for speaker?\",\n",
    "    ]\n",
    "    }\n",
    ")\n",
    "\n",
    "# Model-Based Metrics (uses judge LLM to evaluate response quality)\n",
    "response_quality_metrics = [\n",
    "    \"instruction_following\",  # Does the response answer the question?\n",
    "    \"fluency\",                # Is the response well-written?\n",
    "    \"coherence\",              # Is the response logically structured?\n",
    "    \"safety\",                 # Is the response safe/appropriate?\n",
    "    \"text_quality\",           # Overall text quality\n",
    "    \"verbosity\",              # Is response too long/short?\n",
    "]\n",
    "\n",
    "response_quality_result = EvalTask(\n",
    "    dataset = eval_prompt_sample_dataset,\n",
    "    metrics = response_quality_metrics,\n",
    "    experiment = EXPERIMENT_NAME,\n",
    "    output_uri_prefix = BUCKET_URI + \"/rubric-metric-eval\"\n",
    ").evaluate(\n",
    "    runnable = agent_parsed_outcome_sync, # Generate responses on-the-fly (not BYOD mode)\n",
    "    experiment_run_name=EXPERIMENT_RUN\n",
    ")\n",
    "\n",
    "display_eval_report(response_quality_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4bENwFcd6prX"
   },
   "source": [
    "# Eval #2\n",
    "## Define a Custom Metric\n",
    "\n",
    "According to the [documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/models/determine-eval#model-based-metrics), you can define a prompt template for evaluating whether an AI agent's response follows logically from its actions by setting up criteria and a rating system for this evaluation.\n",
    "\n",
    "Define a `criteria` to set the evaluation guidelines and a `pointwise_rating_rubric` to provide a scoring system (1 or 0). Then use a `PointwiseMetricPromptTemplate` to create the template using these components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = {\n",
    "    \"Request Completeness\": (\n",
    "        \"You are a Quality Assurance Analyst. Your task is to evaluate if an AI agent's response \"\n",
    "        \"completely fulfills a user's shopping query based on a single criterion: Request Completeness.\\n\\n\"\n",
    "        \"Assign a score based on whether the agent provided all the categories of information the user asked for.\\n\\n\"\n",
    "        \"Instructions:\\n\"\n",
    "        \"  - Read the user's question to identify all requested information types (e.g., 'price', 'details').\\n\"\n",
    "        \"  - Analyze the response to see which information types were provided.\\n\"\n",
    "        \"  - If all requested types are present, score '1'. Otherwise, score '0'.\\n\\n\"\n",
    "        \"For example, if the user asks for 'price and details,' the response must contain both a price and \"\n",
    "        \"some form of product details.\"\n",
    "    )\n",
    "}\n",
    "\n",
    "pointwise_rating_rubric = {\n",
    "    \"1\": \"The response provides all the types of information explicitly requested in the question.\",\n",
    "    \"0\": \"The response is missing at least one type of information explicitly requested in the question.\",\n",
    "}\n",
    "\n",
    "response_completeness_prompt_template = PointwiseMetricPromptTemplate(\n",
    "    criteria=criteria,\n",
    "    rating_rubric=pointwise_rating_rubric,\n",
    "    input_variables=[\"prompt\"],\n",
    ")\n",
    "\n",
    "response_completeness_metric = PointwiseMetric(\n",
    "    metric=\"response_completeness\",\n",
    "    metric_prompt_template=response_completeness_prompt_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_dkb4gSn7Ywv"
   },
   "outputs": [],
   "source": [
    "#### Run an evaluation task\n",
    "\n",
    "EXPERIMENT_RUN = f\"response-over-tools-{get_id()}\"\n",
    "\n",
    "response_eval_tool_result = EvalTask(\n",
    "    dataset = eval_sample_dataset,\n",
    "    metrics = [response_completeness_metric],\n",
    "    experiment=EXPERIMENT_NAME,\n",
    "    output_uri_prefix=BUCKET_URI + \"/reasoning-metric-eval\",\n",
    ").evaluate(\n",
    "    runnable = agent_parsed_outcome_sync,\n",
    "    experiment_run_name=EXPERIMENT_RUN\n",
    ")\n",
    "\n",
    "display_eval_report(response_eval_tool_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval #3\n",
    "## Custom Function-Based Metrics\n",
    "\n",
    "Python functions for deterministic agent evaluation without LLM (Judges) calls.\n",
    "\n",
    "### Properties\n",
    "- Deterministic output\n",
    "- Explicit business logic encoding\n",
    "- Standard Python debugging\n",
    "\n",
    "### Custom metric functions must:\n",
    "\n",
    "1. Accept evaluation instance: `def metric_fn(instance: dict) -> dict`\n",
    "2. Return dictionary with metric name and numeric score\n",
    "3. Be wrapped in `CustomMetric` for `EvalTask` integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell defines custom evaluation functions/metrics for agent behavior validation.\n",
    "\n",
    "def tool_count_metric(instance: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Counts the number of tools called in the predicted trajectory.\n",
    "    \n",
    "    Args:\n",
    "        instance: Dictionary containing 'predicted_trajectory' key\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with metric name and score\n",
    "    \"\"\"\n",
    "    trajectory = instance.get(\"predicted_trajectory\", \"\")\n",
    "    \n",
    "    # Handle both string (JSON) and list formats\n",
    "    if isinstance(trajectory, str):\n",
    "        try:\n",
    "            trajectory = json.loads(trajectory)\n",
    "        except json.JSONDecodeError:\n",
    "            trajectory = []\n",
    "    \n",
    "    tool_count = len(trajectory) if isinstance(trajectory, list) else 0\n",
    "    \n",
    "    return {\n",
    "        \"tool_count\": tool_count\n",
    "    }\n",
    "\n",
    "\n",
    "def tool_efficiency_metric(instance: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Measures if the agent uses the minimum necessary tools.\n",
    "    Score of 1 if tool count <= 2, otherwise 0.\n",
    "    \n",
    "    Args:\n",
    "        instance: Dictionary containing 'predicted_trajectory' key\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with metric name and score (0 or 1)\n",
    "    \"\"\"\n",
    "    trajectory = instance.get(\"predicted_trajectory\", \"\")\n",
    "    \n",
    "    if isinstance(trajectory, str):\n",
    "        try:\n",
    "            trajectory = json.loads(trajectory)\n",
    "        except json.JSONDecodeError:\n",
    "            trajectory = []\n",
    "    \n",
    "    tool_count = len(trajectory) if isinstance(trajectory, list) else 0\n",
    "    \n",
    "    # Efficient if using 2 or fewer tools\n",
    "    is_efficient = 1 if tool_count <= 2 else 0\n",
    "    \n",
    "    return {\n",
    "        \"tool_efficiency\": is_efficient\n",
    "    }\n",
    "def response_length_metric(instance: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Measures the length of the response in characters.\n",
    "    \n",
    "    Args:\n",
    "        instance: Dictionary containing 'response' key\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with metric name and score\n",
    "    \"\"\"\n",
    "    response = instance.get(\"response\", \"\")\n",
    "    \n",
    "    # Handle JSON-encoded strings\n",
    "    if isinstance(response, str) and response.startswith('\"'):\n",
    "        try:\n",
    "            response = json.loads(response)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    \n",
    "    response_length = len(str(response))\n",
    "    \n",
    "    return {\n",
    "        \"response_length\": response_length\n",
    "    }\n",
    "\n",
    "\n",
    "def response_conciseness_metric(instance: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Checks if response is concise (under 200 characters).\n",
    "    Score of 1 if concise, 0 otherwise.\n",
    "    \n",
    "    Args:\n",
    "        instance: Dictionary containing 'response' key\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with metric name and score (0 or 1)\n",
    "    \"\"\"\n",
    "    response = instance.get(\"response\", \"\")\n",
    "    \n",
    "    if isinstance(response, str) and response.startswith('\"'):\n",
    "        try:\n",
    "            response = json.loads(response)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    \n",
    "    is_concise = 1 if len(str(response)) <= 200 else 0\n",
    "    \n",
    "    return {\n",
    "        \"response_conciseness\": is_concise\n",
    "    }\n",
    "\n",
    "\n",
    "def numeric_response_metric(instance: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Validates if response contains numeric values (for price queries).\n",
    "    Score of 1 if numeric, 0 otherwise.\n",
    "    \n",
    "    Args:\n",
    "        instance: Dictionary containing 'response' key\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with metric name and score (0 or 1)\n",
    "    \"\"\"\n",
    "    response = instance.get(\"response\", \"\")\n",
    "    \n",
    "    if isinstance(response, str) and response.startswith('\"'):\n",
    "        try:\n",
    "            response = json.loads(response)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    \n",
    "    # Check if response contains digits or is a number\n",
    "    response_str = str(response).strip()\n",
    "    has_number = any(char.isdigit() for char in response_str)\n",
    "    \n",
    "    # Or check if it can be converted to a number\n",
    "    try:\n",
    "        float(response_str)\n",
    "        is_numeric = 1\n",
    "    except ValueError:\n",
    "        is_numeric = 1 if has_number else 0\n",
    "    \n",
    "    return {\n",
    "        \"numeric_response\": is_numeric\n",
    "    }\n",
    "def valid_product_metric(instance: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Validates that the agent only queries valid products from the catalog.\n",
    "    \n",
    "    Args:\n",
    "        instance: Dictionary containing 'predicted_trajectory' key\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with metric name and score (0 or 1)\n",
    "    \"\"\"\n",
    "    VALID_PRODUCTS = {\"smartphone\", \"usb charger\", \"shoes\", \"headphones\", \"speaker\"}\n",
    "    \n",
    "    trajectory = instance.get(\"predicted_trajectory\", \"\")\n",
    "    \n",
    "    if isinstance(trajectory, str):\n",
    "        try:\n",
    "            trajectory = json.loads(trajectory)\n",
    "        except json.JSONDecodeError:\n",
    "            trajectory = []\n",
    "    \n",
    "    # Check all tool inputs for valid product names\n",
    "    all_valid = True\n",
    "    if isinstance(trajectory, list):\n",
    "        for tool_call in trajectory:\n",
    "            if isinstance(tool_call, dict):\n",
    "                tool_input = tool_call.get(\"tool_input\", {})\n",
    "                product_name = tool_input.get(\"product_name\", \"\")\n",
    "                if product_name and product_name not in VALID_PRODUCTS:\n",
    "                    all_valid = False\n",
    "                    break\n",
    "    \n",
    "    return {\n",
    "        \"valid_product\": 1 if all_valid else 0\n",
    "    }\n",
    "\n",
    "\n",
    "def correct_tool_selection_metric(instance: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Validates that the agent uses the correct tool based on the prompt.\n",
    "    - Price queries should use 'get_product_price'\n",
    "    - Details queries should use 'get_product_details'\n",
    "    \n",
    "    Args:\n",
    "        instance: Dictionary containing 'prompt' and 'predicted_trajectory' keys\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with metric name and score (0 or 1)\n",
    "    \"\"\"\n",
    "    prompt = instance.get(\"prompt\", \"\").lower()\n",
    "    trajectory = instance.get(\"predicted_trajectory\", \"\")\n",
    "    \n",
    "    if isinstance(trajectory, str):\n",
    "        try:\n",
    "            trajectory = json.loads(trajectory)\n",
    "        except json.JSONDecodeError:\n",
    "            return {\"correct_tool_selection\": 0}\n",
    "    \n",
    "    if not isinstance(trajectory, list) or len(trajectory) == 0:\n",
    "        return {\"correct_tool_selection\": 0}\n",
    "    \n",
    "    # Extract tool names used\n",
    "    tools_used = [call.get(\"tool_name\", \"\") for call in trajectory if isinstance(call, dict)]\n",
    "    \n",
    "    correct = False\n",
    "    \n",
    "    # Check if correct tools were used based on prompt\n",
    "    if \"price\" in prompt and \"get_product_price\" in tools_used:\n",
    "        correct = True\n",
    "    elif \"details\" in prompt and \"price\" not in prompt and \"get_product_details\" in tools_used:\n",
    "        correct = True\n",
    "    elif \"details\" in prompt and \"price\" in prompt:\n",
    "        # Both tools should be used\n",
    "        correct = \"get_product_details\" in tools_used and \"get_product_price\" in tools_used\n",
    "    \n",
    "    return {\n",
    "        \"correct_tool_selection\": 1 if correct else 0\n",
    "    }\n",
    "\n",
    "\n",
    "def price_range_validation_metric(instance: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Validates that price responses are within expected range ($1-$1000).\n",
    "    \n",
    "    Args:\n",
    "        instance: Dictionary containing 'response' key\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with metric name and score (0 or 1)\n",
    "    \"\"\"\n",
    "    response = instance.get(\"response\", \"\")\n",
    "    \n",
    "    if isinstance(response, str) and response.startswith('\"'):\n",
    "        try:\n",
    "            response = json.loads(response)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    \n",
    "    response_str = str(response).strip()\n",
    "    \n",
    "    # Try to extract numeric value\n",
    "    try:\n",
    "        # Remove currency symbols and convert to float\n",
    "        price = float(response_str.replace(\"$\", \"\").replace(\",\", \"\"))\n",
    "        valid = 1 if 1 <= price <= 1000 else 0\n",
    "    except ValueError:\n",
    "        # If not a price response, consider it valid (not applicable)\n",
    "        valid = 1\n",
    "    \n",
    "    return {\n",
    "        \"price_range_valid\": valid\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define CustomMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap all custom metrics using CustomMetric class\n",
    "custom_metrics = [\n",
    "    # Tool usage metrics\n",
    "    CustomMetric(name=\"tool_count\", metric_function=tool_count_metric),\n",
    "    CustomMetric(name=\"tool_efficiency\", metric_function=tool_efficiency_metric),\n",
    "    \n",
    "    # Response quality metrics\n",
    "    CustomMetric(name=\"response_length\", metric_function=response_length_metric),\n",
    "    CustomMetric(name=\"response_conciseness\", metric_function=response_conciseness_metric),\n",
    "    CustomMetric(name=\"numeric_response\", metric_function=numeric_response_metric),\n",
    "    \n",
    "    # Business logic metrics\n",
    "    CustomMetric(name=\"valid_product\", metric_function=valid_product_metric),\n",
    "    CustomMetric(name=\"correct_tool_selection\", metric_function=correct_tool_selection_metric),\n",
    "    CustomMetric(name=\"price_range_valid\", metric_function=price_range_validation_metric),\n",
    "]\n",
    "\n",
    "print(f\"✓ Created {len(custom_metrics)} custom metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation with custom metrics\n",
    "\n",
    "# Generate a unique experiment run identifier\n",
    "EXPERIMENT_RUN = f\"custom-metrics-eval-{get_id()}\"\n",
    "\n",
    "# Execute the evaluation task with custom deterministic metrics\n",
    "custom_metrics_result = EvalTask(\n",
    "    dataset=eval_prompt_sample_dataset,  # Dataset with only prompts (agent generates responses during eval)\n",
    "    metrics=custom_metrics,  # List of 8 custom function-based metrics defined above\n",
    "    experiment=EXPERIMENT_NAME,  # Vertex AI experiment name for tracking across runs\n",
    "    output_uri_prefix=BUCKET_URI + \"/custom-metrics-eval\",  # GCS path to persist results\n",
    ").evaluate(\n",
    "    runnable=agent_parsed_outcome_sync,  # Wrapper function that invokes the agent for each prompt\n",
    "    experiment_run_name=EXPERIMENT_RUN  # Unique name for this specific evaluation run\n",
    ")\n",
    "\n",
    "# Display summary statistics and row-level metrics in formatted tables\n",
    "display_eval_report(custom_metrics_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed metrics for each row\n",
    "display(Markdown(\"### Detailed Results by Instance\"))\n",
    "display(custom_metrics_result.metrics_table)\n",
    "\n",
    "# Show specific metrics of interest\n",
    "display(Markdown(\"### Key Custom Metrics Summary\"))\n",
    "key_metrics = [\n",
    "    \"tool_efficiency/mean\",\n",
    "    \"correct_tool_selection/mean\", \n",
    "    \"valid_product/mean\",\n",
    "    \"response_conciseness/mean\"\n",
    "]\n",
    "\n",
    "summary_df = pd.DataFrame([\n",
    "    {\"Metric\": metric.replace(\"/mean\", \"\").replace(\"_\", \" \").title(), \n",
    "     \"Score\": custom_metrics_result.summary_metrics.get(metric, \"N/A\")}\n",
    "    for metric in key_metrics\n",
    "])\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIppkS2jq_Dn"
   },
   "source": [
    "## Cleaning up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ox2I3UfRlTOd"
   },
   "outputs": [],
   "source": [
    "### Optional: Delete Experiment Artifacts\n",
    "delete_experiment = True\n",
    "\n",
    "if delete_experiment:\n",
    "    try:\n",
    "        experiment = aiplatform.Experiment(EXPERIMENT_NAME)\n",
    "        experiment.delete(delete_backing_tensorboard_runs=True)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank you"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "evaluating_adk_agent.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
