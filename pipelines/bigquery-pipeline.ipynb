{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e52ebc4f",
   "metadata": {},
   "source": [
    "# Cross-Project BigQuery Access: IAM Permissions Lab\n",
    "\n",
    "This notebook demonstrates how to configure and troubleshoot IAM permissions when a Vertex AI Pipeline needs to access BigQuery tables across different GCP projects.\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "**Project 1** (`matt-demos`): Pipeline execution project where Vertex AI Pipeline runs  \n",
    "**Project 2** (`matt-demos-secondary`): Data storage project containing the BigQuery table  \n",
    "**Service Account**: `bby-lab@matt-demos.iam.gserviceaccount.com`\n",
    "\n",
    "## Required IAM Roles\n",
    "\n",
    "### Project 1: Pipeline Project (`matt-demos`)\n",
    "Service Account: `bby-lab@matt-demos.iam.gserviceaccount.com`\n",
    "- BigQuery Job User (create and run query jobs)\n",
    "- Storage Object Admin (access pipeline artifacts)\n",
    "- Vertex AI User (submit and manage pipelines)\n",
    "\n",
    "### Project 2: Data Project (`matt-demos-secondary`)\n",
    "Service Account: `bby-lab@matt-demos.iam.gserviceaccount.com`\n",
    "- BigQuery Data Viewer (read table data)\n",
    "\n",
    "## Cross-Project BigQuery Access Flow\n",
    "\n",
    "1. **Pipeline Start**: Vertex AI Pipeline initiates in Project 1\n",
    "2. **Component Initialization**: Pipeline component begins execution\n",
    "3. **Client Setup**: BigQuery client initialized with billing project set to Project 1\n",
    "4. **Job Submission**: Query job submitted to Project 1 (requires `bigquery.jobs.create`)\n",
    "5. **SQL Execution**: Query references table in Project 2\n",
    "   ```sql\n",
    "   SELECT * FROM `matt-demos-secondary.dataset_id.table_id`\n",
    "   ```\n",
    "6. **Permission Check**: BigQuery validates service account permissions in Project 2\n",
    "7. **Data Transfer**: Data streams from Project 2 to Project 1 pipeline component\n",
    "8. **Completion**: Component processes data and completes\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "- The **billing project** (Project 1) is where query jobs are executed and billed\n",
    "- The **data project** (Project 2) is where the source table resides\n",
    "- The service account requires appropriate permissions in **both projects**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bc9e0f-c643-4e77-8009-becb318bc7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies for Kubeflow Pipelines, BigQuery, and Vertex AI\n",
    "%pip install --upgrade kfp google-cloud-bigquery \\\n",
    "    bigframes google-cloud-aiplatform \\\n",
    "    db-dtypes google-cloud-pipeline-components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5adf99c6-b2a4-46e4-b377-6d2ebc70c3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import time\n",
    "from typing import NamedTuple\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import logging\n",
    "\n",
    "# Kubeflow Pipelines SDK\n",
    "from kfp import dsl, compiler\n",
    "\n",
    "# Google Cloud services\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import bigquery\n",
    "import bigframes.pandas as bpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7823efc9-888a-41d3-adef-b04c431fcbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters for the pipeline and BigQuery resources\n",
    "\n",
    "# Project configuration\n",
    "PROJECT_ID = \"matt-demos\"  # Primary project where pipeline executes\n",
    "PROJECT_ID_2 = \"matt-demos-secondary\"  # Secondary project containing BigQuery data\n",
    "LOCATION = \"us-central1\"  # GCP region for pipeline execution\n",
    "\n",
    "# Storage configuration\n",
    "STAGING_BUCKET = \"gs://b1bd1e40-2c20-433d-83a1-9b691be93b38\"  # GCS bucket for pipeline artifacts\n",
    "PIPELINE_ROOT = os.path.join(STAGING_BUCKET, \"pipeline-root\")\n",
    "\n",
    "# BigQuery table configuration\n",
    "DATASET_ID = \"synthetic_data\"  # Dataset name in Project 2\n",
    "TABLE_ID = \"sample_products\"  # Table name in Project 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479b3221",
   "metadata": {},
   "source": [
    "## Setup: Create Sample BigQuery Dataset\n",
    "\n",
    "This section creates a sample BigQuery table with synthetic product data in the secondary project for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a6191f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BigQuery client for the secondary project\n",
    "client = bigquery.Client(project=PROJECT_ID_2, location=\"US\")\n",
    "\n",
    "# Construct fully qualified table identifier\n",
    "FULL_TABLE_ID = f\"{PROJECT_ID_2}.{DATASET_ID}.{TABLE_ID}\"\n",
    "\n",
    "# Create dataset if it doesn't already exist\n",
    "dataset = bigquery.Dataset(f\"{PROJECT_ID_2}.{DATASET_ID}\")\n",
    "dataset.location = \"US\"\n",
    "dataset = client.create_dataset(dataset, exists_ok=True)\n",
    "print(f\"Dataset {DATASET_ID} created or already exists in US location\")\n",
    "\n",
    "# Define schema for the products table\n",
    "schema = [\n",
    "    bigquery.SchemaField(\"product_id\", \"INTEGER\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"product_name\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"category\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"price\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"in_stock\", \"BOOLEAN\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"created_date\", \"TIMESTAMP\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"description\", \"STRING\", mode=\"NULLABLE\"),\n",
    "]\n",
    "\n",
    "# Create table with defined schema\n",
    "table = bigquery.Table(FULL_TABLE_ID, schema=schema)\n",
    "table = client.create_table(table, exists_ok=True)\n",
    "print(f\"Table {FULL_TABLE_ID} created\")\n",
    "\n",
    "# Generate synthetic product data\n",
    "categories = [\"Electronics\", \"Clothing\", \"Books\", \"Home & Garden\", \"Toys\", \"Sports\"]\n",
    "products = [\n",
    "    \"Laptop\", \"T-Shirt\", \"Novel\", \"Chair\", \"Action Figure\", \"Basketball\",\n",
    "    \"Smartphone\", \"Jeans\", \"Cookbook\", \"Lamp\", \"Puzzle\", \"Tennis Racket\",\n",
    "    \"Tablet\", \"Dress\", \"Magazine\", \"Rug\", \"Board Game\", \"Golf Clubs\",\n",
    "    \"Headphones\", \"Sweater\", \"Comic Book\", \"Vase\", \"LEGO Set\", \"Yoga Mat\"\n",
    "]\n",
    "\n",
    "rows_to_insert = []\n",
    "base_date = datetime.now() - timedelta(days=365)\n",
    "\n",
    "# Create 100 rows of synthetic product data\n",
    "for i in range(1, 101):\n",
    "    product_name = random.choice(products) + f\" Model-{i}\"\n",
    "    category = random.choice(categories)\n",
    "    price = round(random.uniform(9.99, 999.99), 2)\n",
    "    in_stock = random.choice([True, False])\n",
    "    created_date = base_date + timedelta(days=random.randint(0, 365))\n",
    "    description = f\"High-quality {product_name.lower()} from the {category} category\"\n",
    "    \n",
    "    rows_to_insert.append({\n",
    "        \"product_id\": i,\n",
    "        \"product_name\": product_name,\n",
    "        \"category\": category,\n",
    "        \"price\": price,\n",
    "        \"in_stock\": in_stock,\n",
    "        \"created_date\": created_date.isoformat(),\n",
    "        \"description\": description\n",
    "    })\n",
    "\n",
    "# Insert rows into BigQuery table\n",
    "errors = client.insert_rows_json(FULL_TABLE_ID, rows_to_insert)\n",
    "\n",
    "if errors:\n",
    "    print(f\"Errors occurred while inserting rows: {errors}\")\n",
    "else:\n",
    "    print(f\"Successfully inserted {len(rows_to_insert)} rows into {FULL_TABLE_ID}\")\n",
    "    \n",
    "# Query to verify data insertion and display summary statistics\n",
    "query = f\"\"\"\n",
    "    SELECT \n",
    "        category,\n",
    "        COUNT(*) as product_count,\n",
    "        AVG(price) as avg_price,\n",
    "        SUM(CASE WHEN in_stock THEN 1 ELSE 0 END) as in_stock_count\n",
    "    FROM `{FULL_TABLE_ID}`\n",
    "    GROUP BY category\n",
    "    ORDER BY product_count DESC\n",
    "\"\"\"\n",
    "\n",
    "query_job = client.query(query)\n",
    "results = query_job.result()\n",
    "\n",
    "print(\"\\nData Summary:\")\n",
    "for row in results:\n",
    "    print(f\"Category: {row.category}, Count: {row.product_count}, Avg Price: ${row.avg_price:.2f}, In Stock: {row.in_stock_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c124a89",
   "metadata": {},
   "source": [
    "## Pipeline Definition\n",
    "\n",
    "This section defines the Vertex AI Pipeline components and the pipeline itself. Two approaches are demonstrated for reading BigQuery data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadfba6e",
   "metadata": {},
   "source": [
    "### Component 1: BigFrames Approach (Recommended)\n",
    "\n",
    "Uses BigFrames for lazy evaluation and server-side processing. This approach is more efficient for large datasets as it minimizes data transfer and leverages BigQuery's distributed compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afc4ea86-3153-4496-9833-dc08ec7c0097",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image=\"python:3.12\", packages_to_install=[\"bigframes\", \"db-dtypes\"])\n",
    "def read_bigframes_bigquery_table(\n",
    "    project_id: str, \n",
    "    billing_project_id: str, \n",
    "    dataset_id: str, \n",
    "    table_id: str\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Reads a BigQuery table using BigFrames with lazy evaluation.\n",
    "    \n",
    "    Args:\n",
    "        project_id: GCP project containing the BigQuery table (data project)\n",
    "        billing_project_id: GCP project to bill query costs to (pipeline project)\n",
    "        dataset_id: BigQuery dataset name\n",
    "        table_id: BigQuery table name\n",
    "    \n",
    "    Returns:\n",
    "        String representation of the DataFrame\n",
    "    \"\"\"\n",
    "    import logging\n",
    "    import sys\n",
    "    import bigframes.pandas as bpd\n",
    "\n",
    "    # Construct fully qualified table identifier\n",
    "    full_table_id = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "    # Configure logging to capture detailed execution information\n",
    "    logging.basicConfig(level=logging.DEBUG)\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # Helper function to ensure logs are flushed immediately\n",
    "    def flush_logs():\n",
    "        sys.stdout.flush()\n",
    "        sys.stderr.flush()\n",
    "        for handler in logger.handlers:\n",
    "            handler.flush()\n",
    "    \n",
    "    try:\n",
    "        # Configure BigFrames to use the billing project for query execution\n",
    "        bpd.options.bigquery.project = billing_project_id\n",
    "        bpd.options.bigquery.location = \"US\"\n",
    "\n",
    "        # Construct SQL query to read entire table\n",
    "        sql = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM `{full_table_id}`\n",
    "        \"\"\"\n",
    "\n",
    "        # Execute query using BigFrames (lazy evaluation)\n",
    "        # Query is not executed until data is materialized\n",
    "        all_features_bf = bpd.read_gbq(sql, use_cache=True)\n",
    "\n",
    "        logger.debug(f\"Shape of data loaded: {all_features_bf.shape}\")\n",
    "        flush_logs()\n",
    "\n",
    "        # Convert BigFrames DataFrame to pandas (triggers actual query execution)\n",
    "        all_features = all_features_bf.to_pandas()\n",
    "\n",
    "        # Log summary information\n",
    "        logger.info(f\"\\nLoaded {all_features.shape[0]} rows and {all_features.shape[1]} columns from {full_table_id}\")\n",
    "        \n",
    "        # Return DataFrame as string representation\n",
    "        dataframe_str = all_features.to_string()\n",
    "        return dataframe_str\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading table: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd80b9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image=\"python:3.12\", packages_to_install=[\"google-cloud-bigquery\", \"db-dtypes\"])\n",
    "def read_bigquery_table(\n",
    "    project_id: str, \n",
    "    billing_project_id: str, \n",
    "    dataset_id: str, \n",
    "    table_id: str\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Reads a BigQuery table using the traditional google-cloud-bigquery library.\n",
    "    \n",
    "    Args:\n",
    "        project_id: GCP project containing the BigQuery table (data project)\n",
    "        billing_project_id: GCP project to bill query costs to (pipeline project)\n",
    "        dataset_id: BigQuery dataset name\n",
    "        table_id: BigQuery table name\n",
    "    \n",
    "    Returns:\n",
    "        String representation of the DataFrame\n",
    "    \"\"\"\n",
    "    import logging\n",
    "    import sys\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    # Construct fully qualified table identifier\n",
    "    full_table_id = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "    # Configure logging to capture detailed execution information\n",
    "    logging.basicConfig(level=logging.DEBUG)\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # Helper function to ensure logs are flushed immediately\n",
    "    def flush_logs():\n",
    "        sys.stdout.flush()\n",
    "        sys.stderr.flush()\n",
    "        for handler in logger.handlers:\n",
    "            handler.flush()\n",
    "    \n",
    "    try:\n",
    "        # Configure query job to enable result caching\n",
    "        job_config = bigquery.QueryJobConfig(use_query_cache=True)\n",
    "\n",
    "        # Construct SQL query to read entire table\n",
    "        sql = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM `{full_table_id}`\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize BigQuery client with billing project and execute query\n",
    "        # Results are immediately downloaded and converted to pandas DataFrame\n",
    "        all_features = bigquery.Client(\n",
    "            location=\"US\", \n",
    "            project=billing_project_id\n",
    "        ).query(sql, job_config=job_config).to_dataframe()\n",
    "\n",
    "        logger.debug(f\"Shape of data loaded: {all_features.shape}\")\n",
    "        flush_logs()\n",
    "\n",
    "        # Log summary information\n",
    "        logger.info(f\"\\nLoaded {all_features.shape[0]} rows and {all_features.shape[1]} columns from {full_table_id}\")\n",
    "        \n",
    "        # Return DataFrame as string representation\n",
    "        dataframe_str = all_features.to_string()\n",
    "        return dataframe_str\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading table: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e339d5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"bigquery-cross-project-access-pipeline\", \n",
    "    pipeline_root=PIPELINE_ROOT\n",
    ")\n",
    "def pipeline(\n",
    "    project_id: str, \n",
    "    billing_project_id: str, \n",
    "    location: str, \n",
    "    dataset_id: str, \n",
    "    table_id: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Pipeline that demonstrates reading BigQuery data across projects.\n",
    "    \n",
    "    Args:\n",
    "        project_id: Project containing the BigQuery table\n",
    "        billing_project_id: Project to bill query execution to\n",
    "        location: GCP region for pipeline execution\n",
    "        dataset_id: BigQuery dataset name\n",
    "        table_id: BigQuery table name\n",
    "    \"\"\"\n",
    "    \n",
    "    # BigFrames approach (recommended for large datasets)\n",
    "    read_bigframes = read_bigframes_bigquery_table(\n",
    "        project_id=project_id,\n",
    "        billing_project_id=billing_project_id,\n",
    "        dataset_id=dataset_id,\n",
    "        table_id=table_id\n",
    "    )\n",
    "\n",
    "    # Traditional approach (simpler but less efficient for large data)\n",
    "    read_traditional = read_bigquery_table(\n",
    "        project_id=project_id,\n",
    "        billing_project_id=billing_project_id,\n",
    "        dataset_id=dataset_id,\n",
    "        table_id=table_id\n",
    "    )\n",
    "    \n",
    "# Compile the pipeline to JSON specification\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path=\"bigquery_cross_project_access_pipeline.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "803102b9-4d7b-4e97-9f50-bdafb1879c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/941046250687/locations/us-central1/pipelineJobs/bigquery-cross-project-access-pipeline-20251030173541\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/941046250687/locations/us-central1/pipelineJobs/bigquery-cross-project-access-pipeline-20251030173541')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/bigquery-cross-project-access-pipeline-20251030173541?project=941046250687\n",
      "Job name: projects/941046250687/locations/us-central1/pipelineJobs/bigquery-cross-project-access-pipeline-20251030173541\n",
      "Console URL: https://console.cloud.google.com/vertex-ai/pipelines/runs/bigquery-cross-project-access-pipeline-20251030173541?project=matt-demos\n"
     ]
    }
   ],
   "source": [
    "# Initialize Vertex AI SDK with primary project configuration\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "# Define path to compiled pipeline specification\n",
    "package_path = \"bigquery_cross_project_access_pipeline.json\"\n",
    "\n",
    "# Create and configure the pipeline job\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=f\"bigquery-cross-project-access-{int(time.time())}\",\n",
    "    template_path=package_path,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={\n",
    "        \"project_id\": PROJECT_ID_2,  # Data project containing BigQuery table\n",
    "        \"billing_project_id\": PROJECT_ID,  # Billing project for query execution\n",
    "        \"location\": LOCATION,\n",
    "        \"dataset_id\": DATASET_ID,\n",
    "        \"table_id\": TABLE_ID\n",
    "    },\n",
    "    enable_caching=False,  # Disable caching to ensure fresh execution\n",
    ")\n",
    "\n",
    "# Submit the pipeline job with specified service account\n",
    "# Note: Service account must have required permissions in both projects\n",
    "job.submit(service_account=\"bby-lab@matt-demos.iam.gserviceaccount.com\")\n",
    "\n",
    "# Display job information and console link\n",
    "print(f\"Job name: {job.resource_name}\")\n",
    "print(f\"Console URL: https://console.cloud.google.com/vertex-ai/pipelines/runs/{job.name.split('/')[-1]}?project={PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf696e0a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fb1a7c",
   "metadata": {},
   "source": [
    "## Technical Reference: BigQuery Data Loading Approaches\n",
    "\n",
    "This section provides a detailed technical comparison of the two methods for loading BigQuery data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c56d5e-fa96-4bcc-b359-3ae8fd48eaac",
   "metadata": {},
   "source": [
    "### Comparison Table\n",
    "\n",
    "| Aspect | `google-cloud-bigquery` | `bigframes` |\n",
    "|--------|------------------------|-------------|\n",
    "| **Execution Model** | Eager | Lazy |\n",
    "| **Processing Location** | Client (local Python) | Server (BigQuery) |\n",
    "| **Memory Constraint** | Local RAM | None |\n",
    "| **Data Transfer** | Full dataset | Requested results only |\n",
    "| **Query Optimization** | Manual | Automatic (query fusion) |\n",
    "| **Scalability** | RAM-limited | Cloud-scale |\n",
    "| **Temporary Tables** | Created for large results | Minimized |\n",
    "\n",
    "### Query Optimization Example\n",
    "\n",
    "**Traditional Approach** (3 network roundtrips):\n",
    "```python\n",
    "df = client.query(\"SELECT * FROM table\").to_dataframe()  # Downloads 100 rows\n",
    "filtered = df[df['price'] > 100]                         # Filters in Python\n",
    "result = filtered.groupby('category')['price'].mean()    # Aggregates in Python\n",
    "```\n",
    "\n",
    "**BigFrames Approach** (1 optimized query):\n",
    "```python\n",
    "df = bpd.read_gbq(\"SELECT * FROM table\")              # No execution\n",
    "filtered = df[df['price'] > 100]                      # Adds to computation graph\n",
    "result = filtered.groupby('category')['price'].mean() # Compiles to SQL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ba7d91-31a9-4d74-9cf2-8b3628c50bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m133",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m133"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
