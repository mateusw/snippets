{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9090b035-c3ef-49a3-9f42-f4e27222a8e2",
   "metadata": {},
   "source": [
    "# Vertex AI Feature Store Migration Guide\n",
    "## Legacy Feature Store (1.0) → Feature Store 2.0\n",
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "\n",
    "This notebook provides a complete migration path from Vertex AI Feature Store Legacy (1.0) to the new Feature Store 2.0 architecture. The migration uses BigQuery as an intermediate storage layer to transfer feature data between systems.\n",
    "\n",
    "**Migration Flow:**  \n",
    "`Legacy Feature Store` → `BigQuery Export` → `Feature Store 2.0`\n",
    "\n",
    "---\n",
    "\n",
    "### Quick Start\n",
    "\n",
    "1. Configure project settings in **Configuration Settings** cell\n",
    "2. Run all cells sequentially\n",
    "3. Monitor sync completion and validate results\n",
    "\n",
    "---\n",
    "\n",
    "### Operations Summary\n",
    "\n",
    "| Step | Operation | Description | Estimated Time |\n",
    "|------|-----------|-------------|----------------|\n",
    "| 1 | **Install Packages** | Install required Google Cloud libraries | 1-2 min |\n",
    "| 2 | **Initialize Clients** | Create API clients for Feature Store, BigQuery, and Feature Store 2.0 | < 1 min |\n",
    "| 3 | **Export to BigQuery** | Export all features from Legacy EntityType to BigQuery table | 2-10 min |\n",
    "| 4 | **Validate Export** | Verify row count, schema, and data integrity | < 1 min |\n",
    "| 5 | **Create Online Store** | Provision Feature Store 2.0 with Bigtable backend | 5-10 min |\n",
    "| 6 | **Create FeatureView** | Link BigQuery table to Feature Store 2.0 | 1-2 min |\n",
    "| 7 | **Trigger Sync** | Populate Feature Store 2.0 with exported data | 2-15 min |\n",
    "| 8 | **Check Sync Status** | Review sync history and validate completion | < 1 min |\n",
    "\n",
    "**Total Time:** 15-45 minutes (varies by data volume)\n",
    "\n",
    "---\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- ✓ GCP project with Vertex AI API enabled\n",
    "- ✓ Existing Legacy Feature Store (1.0) with EntityType\n",
    "- ✓ BigQuery dataset for intermediate storage\n",
    "- ✓ IAM permissions: `Vertex AI Administrator`, `BigQuery User`\n",
    "- ✓ Sufficient quota for Bigtable nodes in target region\n",
    "\n",
    "---\n",
    "\n",
    "### Important Notes\n",
    "\n",
    "- This is a **reference implementation** for migration purposes\n",
    "- The migration is **non-destructive** (legacy store remains unchanged)\n",
    "- Feature Store 2.0 supports scheduled syncs via cron configuration\n",
    "- Embedding management can be enabled for vector similarity search\n",
    "\n",
    "---\n",
    "\n",
    "### References\n",
    "\n",
    "- [Feature Store Migration Documentation](https://cloud.google.com/vertex-ai/docs/featurestore#migrate)\n",
    "- [Vertex AI Samples Repository](https://github.com/GoogleCloudPlatform/vertex-ai-samples)\n",
    "- [Feature Store 2.0 Overview](https://cloud.google.com/vertex-ai/docs/featurestore/latest/overview)\n",
    "\n",
    "---\n",
    "\n",
    "**Questions?:** mateuswagner@google.com  \n",
    "**Last Updated:** 2025-11  \n",
    "**Status:** Not an Official Google Product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14544694-c709-46a6-8187-9c5744f1bfb9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 1: Export Legacy Feature Store to BigQuery\n",
    "\n",
    "### Step 1: Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea483113-3a36-4d18-b06f-475381289671",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade google-cloud-aiplatform google-cloud-bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1cf42c-6969-4037-9a99-2d7ff86a2992",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "from typing import Optional\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.aiplatform_v1beta1 import (\n",
    "    FeaturestoreServiceClient,\n",
    "    FeatureValueDestination,\n",
    "    FeatureSelector,\n",
    "    ExportFeatureValuesRequest,\n",
    "    ListFeatureViewSyncsRequest,\n",
    "    types,\n",
    ")\n",
    "from google.cloud.aiplatform_v1beta1 import (\n",
    "    FeatureOnlineStoreAdminServiceClient,\n",
    "    FeatureOnlineStoreServiceClient,\n",
    ")\n",
    "from google.cloud.aiplatform_v1beta1.types import (\n",
    "    feature_online_store as feature_online_store_pb2,\n",
    "    feature_online_store_admin_service as feature_online_store_admin_service_pb2,\n",
    "    feature_online_store_service as feature_online_store_service_pb2,\n",
    "    feature_view as feature_view_pb2,\n",
    ")\n",
    "from google.api_core import exceptions\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google.cloud.aiplatform_v1beta1 import (\n",
    "    FeaturestoreServiceClient,\n",
    "    Featurestore,\n",
    "    EntityType,\n",
    "    Feature,\n",
    ")\n",
    "from google.cloud.aiplatform_v1beta1.types import (\n",
    "    featurestore_service as featurestore_service_pb2,\n",
    "    entity_type as entity_type_pb2,\n",
    "    feature as feature_pb2,\n",
    ")\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "472a0ad7-a566-4378-82a3-54fbbb6c5c3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configuration Settings\n",
    "# Update these values according to your GCP environment\n",
    "\n",
    "# GCP Project Configuration\n",
    "PROJECT_ID = \"matt-demos\"\n",
    "REGION_ID = \"us-central1\"\n",
    "\n",
    "# Legacy Feature Store Configuration (Source)\n",
    "FEATURE_STORE_ID = \"legacy_fs_1\"\n",
    "ENTITYTYPE_ID = \"benchmark_3emb_1000xfloat64\"\n",
    "\n",
    "# BigQuery Configuration (Intermediate Storage)\n",
    "BIGQUERY_DEST_DATASET_ID = \"featurestore_ds\"\n",
    "\n",
    "# Feature Store 2.0 Configuration (Destination)\n",
    "FEATURE_ONLINE_STORE_ID = \"featurestore_4\"\n",
    "FEATURE_ONLINE_STORE_EMBEDDING_MNGMT = True\n",
    "\n",
    "# Bigtable Auto-scaling Configuration\n",
    "BIGTABLE_MIN_NODE_COUNT = 1\n",
    "BIGTABLE_MAX_NODE_COUNT = 2\n",
    "BIGTABLE_CPU_UTILIZATION_TARGET = 50\n",
    "\n",
    "# Sync Schedule (Cron format with timezone)\n",
    "# Example: Run at 56 minutes past every hour, Pacific Time\n",
    "CRON_SCHEDULE = \"TZ=America/Los_Angeles 56 * * * *\"\n",
    "\n",
    "# Operation Timeouts\n",
    "DEFAULT_TIMEOUT = 3600  # 1 hour in seconds\n",
    "SYNC_POLL_INTERVAL = 30  # 30 seconds\n",
    "\n",
    "# Logging Configuration\n",
    "LOG_LEVEL = logging.INFO  # Use logging.DEBUG for troubleshooting\n",
    "\n",
    "logger.setLevel(LOG_LEVEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12d3e826-2d88-46e4-a3c4-0c37a8b90a76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-20 17:18:13,446 - INFO - Configuration initialized successfully\n",
      "2025-11-20 17:18:13,447 - INFO - Source: projects/matt-demos/locations/us-central1/featurestores/legacy_fs_1/entityTypes/benchmark_3emb_1000xfloat64\n",
      "2025-11-20 17:18:13,447 - INFO - Destination: projects/matt-demos/locations/us-central1/featureOnlineStores/featurestore_4/benchmark_3emb_1000xfloat64\n",
      "2025-11-20 17:18:13,447 - INFO - Source: projects/matt-demos/locations/us-central1/featurestores/legacy_fs_1/entityTypes/benchmark_3emb_1000xfloat64\n",
      "2025-11-20 17:18:13,447 - INFO - Destination: projects/matt-demos/locations/us-central1/featureOnlineStores/featurestore_4/benchmark_3emb_1000xfloat64\n"
     ]
    }
   ],
   "source": [
    "# Derived Variables - Do not modify unless you know what you're doing\n",
    "\n",
    "# Parent resource path for the region\n",
    "parent_id = f'projects/{PROJECT_ID}/locations/{REGION_ID}'\n",
    "\n",
    "# Full path to the legacy EntityType\n",
    "entitytype_id_path = f'{parent_id}/featurestores/{FEATURE_STORE_ID}/entityTypes/{ENTITYTYPE_ID}'\n",
    "\n",
    "# Extract short entity type name\n",
    "entity_type_short_id = entitytype_id_path.split(\"/\")[-1]\n",
    "\n",
    "# BigQuery destination URI\n",
    "bigquery_output_url = f'bq://{PROJECT_ID}.{BIGQUERY_DEST_DATASET_ID}.{entity_type_short_id}'\n",
    "\n",
    "# Entity ID column name in BigQuery (automatically generated by export)\n",
    "entity_id_column = f'entity_type_{entity_type_short_id}'\n",
    "\n",
    "# Feature Online Store 2.0 resource paths\n",
    "feature_online_stores_name = f\"{parent_id}/featureOnlineStores/{FEATURE_ONLINE_STORE_ID}\"\n",
    "feature_view_id = entity_type_short_id\n",
    "feature_view_full = f\"{feature_online_stores_name}/featureViews/{feature_view_id}\"\n",
    "\n",
    "logger.info(\"Configuration initialized successfully\")\n",
    "logger.info(f\"Source: {entitytype_id_path}\")\n",
    "logger.info(f\"Destination: {feature_online_stores_name}/{feature_view_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xvwp6k01jt",
   "metadata": {},
   "source": [
    "### Step 0: Create Demo Source Feature Store (Optional)\n",
    "\n",
    "**Run this cell only if you need to create a demo Legacy Feature Store with fake data for testing purposes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hckm0qbj3vp",
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nfrom google.cloud.aiplatform_v1beta1 import (\n    FeaturestoreServiceClient,\n    Featurestore,\n    EntityType,\n    Feature,\n)\nfrom google.cloud.aiplatform_v1beta1.types import (\n    featurestore_service as featurestore_service_pb2,\n    entity_type as entity_type_pb2,\n    feature as feature_pb2,\n    featurestore_online_service as featurestore_online_service_pb2,\n)\nfrom google.protobuf.struct_pb2 import Value, ListValue\n\ntry:\n    logger.info(\"=\" * 80)\n    logger.info(\"CREATING DEMO LEGACY FEATURE STORE\")\n    logger.info(\"=\" * 80)\n    \n    # Initialize Legacy Feature Store client\n    demo_fs_client = FeaturestoreServiceClient(\n        client_options={\"api_endpoint\": f'{REGION_ID}-aiplatform.googleapis.com'}\n    )\n    \n    # Derive parent_id (in case not yet set)\n    parent_id = f'projects/{PROJECT_ID}/locations/{REGION_ID}'\n    \n    # Step 1: Create Legacy Feature Store\n    featurestore_path = f\"{parent_id}/featurestores/{FEATURE_STORE_ID}\"\n    \n    try:\n        existing_fs = demo_fs_client.get_featurestore(name=featurestore_path)\n        logger.info(f\"✓ Feature Store '{FEATURE_STORE_ID}' already exists\")\n    except exceptions.NotFound:\n        logger.info(f\"Creating Legacy Feature Store: {FEATURE_STORE_ID}\")\n        create_fs_op = demo_fs_client.create_featurestore(\n            featurestore_service_pb2.CreateFeaturestoreRequest(\n                parent=parent_id,\n                featurestore_id=FEATURE_STORE_ID,\n                featurestore=Featurestore(\n                    online_serving_config=Featurestore.OnlineServingConfig(\n                        fixed_node_count=1\n                    )\n                )\n            )\n        )\n        create_fs_op.result(timeout=600)\n        logger.info(f\"✓ Feature Store '{FEATURE_STORE_ID}' created\")\n    \n    # Step 2: Create EntityType\n    entitytype_id_path = f'{parent_id}/featurestores/{FEATURE_STORE_ID}/entityTypes/{ENTITYTYPE_ID}'\n    \n    try:\n        existing_et = demo_fs_client.get_entity_type(name=entitytype_id_path)\n        logger.info(f\"✓ EntityType '{ENTITYTYPE_ID}' already exists\")\n    except exceptions.NotFound:\n        logger.info(f\"Creating EntityType: {ENTITYTYPE_ID}\")\n        create_et_op = demo_fs_client.create_entity_type(\n            featurestore_service_pb2.CreateEntityTypeRequest(\n                parent=featurestore_path,\n                entity_type_id=ENTITYTYPE_ID,\n                entity_type=EntityType()\n            )\n        )\n        create_et_op.result(timeout=300)\n        logger.info(f\"✓ EntityType '{ENTITYTYPE_ID}' created\")\n    \n    # Step 3: Create Features (3 embeddings with 1000 float64 dimensions each)\n    feature_specs = [\n        (\"embedding_1\", \"DOUBLE\", 1000),\n        (\"embedding_2\", \"DOUBLE\", 1000),\n        (\"embedding_3\", \"DOUBLE\", 1000),\n    ]\n    \n    for feature_name, value_type, array_size in feature_specs:\n        feature_path = f\"{entitytype_id_path}/features/{feature_name}\"\n        \n        try:\n            demo_fs_client.get_feature(name=feature_path)\n            logger.info(f\"✓ Feature '{feature_name}' already exists\")\n        except exceptions.NotFound:\n            logger.info(f\"Creating feature: {feature_name} (ARRAY<{value_type}>[{array_size}])\")\n            \n            # Create feature with array type\n            create_feature_op = demo_fs_client.create_feature(\n                featurestore_service_pb2.CreateFeatureRequest(\n                    parent=entitytype_id_path,\n                    feature_id=feature_name,\n                    feature=Feature(\n                        value_type=Feature.ValueType.DOUBLE_ARRAY,\n                    )\n                )\n            )\n            create_feature_op.result(timeout=300)\n            logger.info(f\"✓ Feature '{feature_name}' created\")\n    \n    # Step 4: Generate and ingest fake embedding data\n    logger.info(\"\")\n    logger.info(\"Generating fake embedding data...\")\n    \n    NUM_ENTITIES = 100  # Generate 100 sample entities\n    EMBEDDING_DIM = 1000\n    \n    # Generate random entity IDs\n    entity_ids = [f\"entity_{i:04d}\" for i in range(NUM_ENTITIES)]\n    \n    # Generate random embeddings (normalized to unit vectors for realism)\n    embeddings_data = []\n    for entity_id in entity_ids:\n        # Create 3 random embeddings\n        emb1 = np.random.randn(EMBEDDING_DIM).astype(np.float64)\n        emb2 = np.random.randn(EMBEDDING_DIM).astype(np.float64)\n        emb3 = np.random.randn(EMBEDDING_DIM).astype(np.float64)\n        \n        # Normalize to unit vectors\n        emb1 = emb1 / np.linalg.norm(emb1)\n        emb2 = emb2 / np.linalg.norm(emb2)\n        emb3 = emb3 / np.linalg.norm(emb3)\n        \n        embeddings_data.append({\n            \"entity_id\": entity_id,\n            \"embedding_1\": emb1.tolist(),\n            \"embedding_2\": emb2.tolist(),\n            \"embedding_3\": emb3.tolist(),\n        })\n    \n    logger.info(f\"Generated {NUM_ENTITIES} entities with 3 embeddings each ({EMBEDDING_DIM} dimensions)\")\n    \n    # Ingest data in batches using streaming write API\n    logger.info(\"Ingesting data into Legacy Feature Store...\")\n    logger.info(\"Note: Using streaming ingestion for demo purposes\")\n    \n    # Batch ingestion (10 entities at a time)\n    BATCH_SIZE = 10\n    total_batches = (NUM_ENTITIES + BATCH_SIZE - 1) // BATCH_SIZE\n    \n    for batch_idx in range(0, NUM_ENTITIES, BATCH_SIZE):\n        batch_entities = embeddings_data[batch_idx:batch_idx + BATCH_SIZE]\n        \n        # Prepare payloads for this batch\n        payloads = []\n        for entity_data in batch_entities:\n            # Create feature values dictionary using protobuf Value\n            feature_values = {}\n            \n            # Convert embeddings to ListValue\n            for emb_name in [\"embedding_1\", \"embedding_2\", \"embedding_3\"]:\n                list_value = ListValue()\n                list_value.values.extend([Value(number_value=v) for v in entity_data[emb_name]])\n                feature_values[emb_name] = featurestore_online_service_pb2.FeatureValue(\n                    value=Value(list_value=list_value)\n                )\n            \n            payloads.append(\n                featurestore_service_pb2.WriteFeatureValuesPayload(\n                    entity_id=entity_data[\"entity_id\"],\n                    feature_values=feature_values\n                )\n            )\n        \n        # Write this batch\n        demo_fs_client.write_feature_values(\n            featurestore_service_pb2.WriteFeatureValuesRequest(\n                entity_type=entitytype_id_path,\n                payloads=payloads\n            )\n        )\n        \n        current_batch = (batch_idx // BATCH_SIZE) + 1\n        logger.info(f\"  Ingested batch {current_batch}/{total_batches} ({len(batch_entities)} entities)\")\n    \n    logger.info(\"\")\n    logger.info(\"=\" * 80)\n    logger.info(\"✓ DEMO LEGACY FEATURE STORE SETUP COMPLETE\")\n    logger.info(\"=\" * 80)\n    logger.info(f\"Feature Store: {FEATURE_STORE_ID}\")\n    logger.info(f\"EntityType: {ENTITYTYPE_ID}\")\n    logger.info(f\"Features: 3 embeddings × {EMBEDDING_DIM} dimensions\")\n    logger.info(f\"Entities ingested: {NUM_ENTITIES}\")\n    logger.info(\"\")\n    logger.info(\"You can now proceed with the export process below.\")\n    logger.info(\"=\" * 80)\n    \nexcept Exception as e:\n    logger.error(f\"Failed to create demo feature store: {e}\")\n    logger.error(\"You may need to manually create the Legacy Feature Store\")\n    raise"
  },
  {
   "cell_type": "markdown",
   "id": "1e140932-2480-4f08-9562-ab64f23e40cc",
   "metadata": {},
   "source": [
    "### Step 2: Initialize API Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d21e9a-a2c1-4ec7-a9ff-08aa71e97a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Initialize Legacy Feature Store client (v1beta1)\n",
    "    fs_client = FeaturestoreServiceClient(\n",
    "        client_options={\n",
    "            \"api_endpoint\": f'{REGION_ID}-aiplatform.googleapis.com'\n",
    "        }\n",
    "    )\n",
    "    logger.info(\"✓ Legacy Feature Store client initialized\")\n",
    "\n",
    "    # Initialize BigQuery client\n",
    "    bq_client = bigquery.Client(project=PROJECT_ID)\n",
    "    logger.info(\"✓ BigQuery client initialized\")\n",
    "\n",
    "    # Initialize Feature Store 2.0 Admin client\n",
    "    admin_vfs2_client = FeatureOnlineStoreAdminServiceClient(\n",
    "        client_options={\n",
    "            \"api_endpoint\": f'{REGION_ID}-aiplatform.googleapis.com'\n",
    "        }\n",
    "    )\n",
    "    logger.info(\"✓ Feature Store 2.0 Admin client initialized\")\n",
    "    \n",
    "    logger.info(\"All clients initialized successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to initialize clients: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae81cdd-34d1-4f61-9101-47d58a294391",
   "metadata": {},
   "source": [
    "### Step 3: Export EntityType from Legacy Feature Store to BigQuery\n",
    "\n",
    "This step exports all feature values from the legacy EntityType to a BigQuery table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6effe07f-9b92-42ed-9f06-b46395da02fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Configure BigQuery destination\n",
    "    destination = FeatureValueDestination()\n",
    "    destination.bigquery_destination.output_uri = bigquery_output_url\n",
    "    \n",
    "    # Select all features using wildcard matcher\n",
    "    feature_selector = FeatureSelector()\n",
    "    feature_selector.id_matcher.ids = ['*']\n",
    "    \n",
    "    logger.info(f\"Starting export from EntityType: {ENTITYTYPE_ID}\")\n",
    "    logger.info(f\"Destination: {bigquery_output_url}\")\n",
    "    logger.info(\"This may take several minutes depending on data size...\")\n",
    "    \n",
    "    # Start export operation\n",
    "    operation = fs_client.export_feature_values(\n",
    "        ExportFeatureValuesRequest(\n",
    "            entity_type=entitytype_id_path,\n",
    "            destination=destination,\n",
    "            feature_selector=feature_selector,\n",
    "            full_export=types.ExportFeatureValuesRequest.FullExport()\n",
    "        ),\n",
    "        timeout=DEFAULT_TIMEOUT\n",
    "    )\n",
    "    \n",
    "    # Wait for export to complete\n",
    "    result = operation.result()\n",
    "    \n",
    "    logger.info(\"✓ Export completed successfully\")\n",
    "    logger.info(f\"Entity type '{entity_type_short_id}' from feature store '{FEATURE_STORE_ID}'\")\n",
    "    logger.info(f\"Exported to BigQuery: {bigquery_output_url}\")\n",
    "    \n",
    "except exceptions.NotFound as e:\n",
    "    logger.error(f\"Resource not found: {e}\")\n",
    "    logger.error(\"Please verify FEATURE_STORE_ID and ENTITYTYPE_ID are correct\")\n",
    "    raise\n",
    "except exceptions.PermissionDenied as e:\n",
    "    logger.error(f\"Permission denied: {e}\")\n",
    "    logger.error(\"Ensure you have the necessary IAM permissions\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    logger.error(f\"Export failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b232e0-ba4d-4edb-8044-5eaa767e7db1",
   "metadata": {},
   "source": [
    "### Step 4: Validate Exported Data\n",
    "\n",
    "Verify the export by checking row count and schema of the BigQuery table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954afc15-fd56-40cd-829a-f803663ec5b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    table_ref = f'{PROJECT_ID}.{BIGQUERY_DEST_DATASET_ID}.{ENTITYTYPE_ID}'\n",
    "    table = bq_client.get_table(table_ref)\n",
    "    \n",
    "    logger.info(\"=\" * 80)\n",
    "    logger.info(\"EXPORT VALIDATION\")\n",
    "    logger.info(\"=\" * 80)\n",
    "    logger.info(f\"Table: {table_ref}\")\n",
    "    logger.info(f\"Total Rows: {table.num_rows:,}\")\n",
    "    logger.info(f\"Total Columns: {len(table.schema)}\")\n",
    "    logger.info(f\"Table Size: {table.num_bytes / (1024**3):.2f} GB\")\n",
    "    logger.info(f\"Created: {table.created}\")\n",
    "    logger.info(f\"Modified: {table.modified}\")\n",
    "    logger.info(\"\")\n",
    "    logger.info(\"Schema (first 10 columns):\")\n",
    "    for i, field in enumerate(table.schema[:10]):\n",
    "        logger.info(f\"  {i+1}. {field.name} ({field.field_type})\")\n",
    "    \n",
    "    if len(table.schema) > 10:\n",
    "        logger.info(f\"  ... and {len(table.schema) - 10} more columns\")\n",
    "    \n",
    "    logger.info(\"=\" * 80)\n",
    "    \n",
    "    # Validate entity ID column exists\n",
    "    column_names = [field.name for field in table.schema]\n",
    "    if entity_id_column not in column_names:\n",
    "        logger.warning(f\"Expected entity ID column '{entity_id_column}' not found!\")\n",
    "        logger.info(f\"Available columns: {', '.join(column_names[:5])}...\")\n",
    "    else:\n",
    "        logger.info(f\"✓ Entity ID column '{entity_id_column}' found\")\n",
    "        \n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to validate exported table: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b881ce-325a-444b-a9a1-a8f7a8a3bdac",
   "metadata": {},
   "source": [
    "## Part 2: Create Feature Store 2.0 and Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27a3f14-a7fc-48ed-abc4-a87377b6b010",
   "metadata": {},
   "source": [
    "### Step 5: Create Feature Online Store 2.0\n",
    "\n",
    "This creates a new Feature Online Store backed by Bigtable with auto-scaling enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1748b21e-4879-4f18-8e11-d1578f3d8047",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Check if online store already exists\n",
    "    try:\n",
    "        existing_store = admin_vfs2_client.get_feature_online_store(\n",
    "            name=feature_online_stores_name\n",
    "        )\n",
    "        logger.info(f\"✓ Feature Online Store '{FEATURE_ONLINE_STORE_ID}' already exists\")\n",
    "        logger.info(f\"State: {existing_store.state.name}\")\n",
    "        logger.info(\"Skipping creation step\")\n",
    "        \n",
    "    except exceptions.NotFound:\n",
    "        logger.info(f\"Creating Feature Online Store: {FEATURE_ONLINE_STORE_ID}\")\n",
    "        logger.info(\"This may take 5-10 minutes...\")\n",
    "        \n",
    "        # Configure Feature Online Store with Bigtable backend\n",
    "        online_store_config = feature_online_store_pb2.FeatureOnlineStore(\n",
    "            bigtable=feature_online_store_pb2.FeatureOnlineStore.Bigtable(\n",
    "                auto_scaling=feature_online_store_pb2.FeatureOnlineStore.Bigtable.AutoScaling(\n",
    "                    min_node_count=BIGTABLE_MIN_NODE_COUNT,\n",
    "                    max_node_count=BIGTABLE_MAX_NODE_COUNT,\n",
    "                    cpu_utilization_target=BIGTABLE_CPU_UTILIZATION_TARGET\n",
    "                )\n",
    "            ),\n",
    "            embedding_management=feature_online_store_pb2.FeatureOnlineStore.EmbeddingManagement(\n",
    "                enabled=FEATURE_ONLINE_STORE_EMBEDDING_MNGMT\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Create the online store\n",
    "        create_store_lro = admin_vfs2_client.create_feature_online_store(\n",
    "            feature_online_store_admin_service_pb2.CreateFeatureOnlineStoreRequest(\n",
    "                parent=parent_id,\n",
    "                feature_online_store_id=FEATURE_ONLINE_STORE_ID,\n",
    "                feature_online_store=online_store_config,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Wait for creation to complete\n",
    "        result = create_store_lro.result()\n",
    "        logger.info(\"✓ Feature Online Store created successfully\")\n",
    "        logger.info(f\"Name: {result.name}\")\n",
    "        logger.info(f\"State: {result.state.name}\")\n",
    "        \n",
    "except exceptions.AlreadyExists:\n",
    "    logger.info(f\"✓ Feature Online Store '{FEATURE_ONLINE_STORE_ID}' already exists\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to create Feature Online Store: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4068067-56a4-4c5c-b77f-81b68fd45e8a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 6: Create FeatureView\n",
    "\n",
    "A FeatureView links the BigQuery table to the Feature Online Store and defines the sync schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7ab023-c519-466c-bbd6-872e9ba8e772",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Check if FeatureView already exists\n",
    "    try:\n",
    "        existing_view = admin_vfs2_client.get_feature_view(name=feature_view_full)\n",
    "        logger.info(f\"✓ FeatureView '{feature_view_id}' already exists\")\n",
    "        logger.info(f\"State: {existing_view.state.name}\")\n",
    "        logger.info(\"Skipping creation step\")\n",
    "        \n",
    "    except exceptions.NotFound:\n",
    "        logger.info(f\"Creating FeatureView: {feature_view_id}\")\n",
    "        logger.info(f\"Source: {bigquery_output_url}\")\n",
    "        logger.info(f\"Entity ID column: {entity_id_column}\")\n",
    "        logger.info(f\"Sync schedule: {CRON_SCHEDULE}\")\n",
    "        \n",
    "        # Configure BigQuery source\n",
    "        big_query_source = feature_view_pb2.FeatureView.BigQuerySource(\n",
    "            uri=bigquery_output_url,\n",
    "            entity_id_columns=[entity_id_column]\n",
    "        )\n",
    "        \n",
    "        # Configure sync schedule\n",
    "        sync_config = feature_view_pb2.FeatureView.SyncConfig(cron=CRON_SCHEDULE)\n",
    "        \n",
    "        # Create FeatureView\n",
    "        create_view_lro = admin_vfs2_client.create_feature_view(\n",
    "            feature_online_store_admin_service_pb2.CreateFeatureViewRequest(\n",
    "                parent=feature_online_stores_name,\n",
    "                feature_view_id=feature_view_id,\n",
    "                feature_view=feature_view_pb2.FeatureView(\n",
    "                    big_query_source=big_query_source,\n",
    "                    sync_config=sync_config,\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Wait for creation to complete\n",
    "        result = create_view_lro.result()\n",
    "        logger.info(\"✓ FeatureView created successfully\")\n",
    "        logger.info(f\"Name: {result.name}\")\n",
    "        logger.info(f\"State: {result.state.name}\")\n",
    "        \n",
    "except exceptions.AlreadyExists:\n",
    "    logger.info(f\"✓ FeatureView '{feature_view_id}' already exists\")\n",
    "except exceptions.InvalidArgument as e:\n",
    "    logger.error(f\"Invalid argument: {e}\")\n",
    "    logger.error(\"Check that BigQuery table exists and entity_id_column is correct\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to create FeatureView: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6fecf1-4f01-4029-a93e-67eaa31c289e",
   "metadata": {},
   "source": [
    "### Step 7: Trigger On-Demand Sync\n",
    "\n",
    "Start an immediate sync to populate the Feature Online Store with data from BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ced754-3a90-42dd-9b60-3b90523d57a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    logger.info(f\"Starting on-demand sync for FeatureView: {feature_view_id}\")\n",
    "    logger.info(\"This may take several minutes depending on data size...\")\n",
    "    \n",
    "    # Trigger sync\n",
    "    sync_response = admin_vfs2_client.sync_feature_view(\n",
    "        feature_view=feature_view_full\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Sync job started: {sync_response.feature_view_sync}\")\n",
    "    \n",
    "    # Poll for sync completion\n",
    "    poll_count = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while True:\n",
    "        feature_view_sync = admin_vfs2_client.get_feature_view_sync(\n",
    "            name=sync_response.feature_view_sync\n",
    "        )\n",
    "        \n",
    "        # Check if sync has completed\n",
    "        if feature_view_sync.run_time.end_time.seconds > 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            status = \"Succeeded\" if feature_view_sync.final_status.code == 0 else \"Failed\"\n",
    "            \n",
    "            logger.info(\"=\" * 80)\n",
    "            logger.info(f\"SYNC {status.upper()}\")\n",
    "            logger.info(\"=\" * 80)\n",
    "            logger.info(f\"Sync Name: {feature_view_sync.name}\")\n",
    "            logger.info(f\"Status Code: {feature_view_sync.final_status.code}\")\n",
    "            logger.info(f\"Start Time: {feature_view_sync.run_time.start_time}\")\n",
    "            logger.info(f\"End Time: {feature_view_sync.run_time.end_time}\")\n",
    "            logger.info(f\"Duration: {elapsed_time:.1f} seconds\")\n",
    "            \n",
    "            if feature_view_sync.final_status.message:\n",
    "                logger.info(f\"Message: {feature_view_sync.final_status.message}\")\n",
    "            \n",
    "            logger.info(\"=\" * 80)\n",
    "            \n",
    "            if feature_view_sync.final_status.code != 0:\n",
    "                raise Exception(f\"Sync failed: {feature_view_sync.final_status.message}\")\n",
    "            \n",
    "            # Wait a bit for the system to stabilize\n",
    "            logger.info(\"Waiting 30 seconds for system stabilization...\")\n",
    "            time.sleep(30)\n",
    "            break\n",
    "        else:\n",
    "            poll_count += 1\n",
    "            elapsed = time.time() - start_time\n",
    "            logger.info(f\"Sync in progress... ({elapsed:.0f}s elapsed, poll #{poll_count})\")\n",
    "            time.sleep(SYNC_POLL_INTERVAL)\n",
    "            \n",
    "except Exception as e:\n",
    "    logger.error(f\"Sync operation failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ba1fc2-a305-46fa-b540-803af6942bf0",
   "metadata": {},
   "source": [
    "### Step 8: Check Sync History\n",
    "\n",
    "View all sync operations for this FeatureView."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36606ed6-a209-40da-b1e0-37ba4c6df4e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    logger.info(\"Fetching sync history...\")\n",
    "    \n",
    "    request = ListFeatureViewSyncsRequest(parent=feature_view_full)\n",
    "    page_result = admin_vfs2_client.list_feature_view_syncs(request=request)\n",
    "    \n",
    "    syncs = list(page_result)\n",
    "    \n",
    "    logger.info(\"=\" * 80)\n",
    "    logger.info(f\"SYNC HISTORY ({len(syncs)} total syncs)\")\n",
    "    logger.info(\"=\" * 80)\n",
    "    \n",
    "    for i, sync in enumerate(syncs, 1):\n",
    "        status = \"Success\" if sync.final_status.code == 0 else \"Failed\"\n",
    "        duration = \"N/A\"\n",
    "        \n",
    "        if sync.run_time.end_time.seconds > 0:\n",
    "            duration_seconds = (\n",
    "                sync.run_time.end_time.seconds - sync.run_time.start_time.seconds\n",
    "            )\n",
    "            duration = f\"{duration_seconds}s\"\n",
    "        \n",
    "        logger.info(f\"\\n{i}. {sync.name.split('/')[-1]}\")\n",
    "        logger.info(f\"   Status: {status} (code: {sync.final_status.code})\")\n",
    "        logger.info(f\"   Start: {sync.run_time.start_time}\")\n",
    "        logger.info(f\"   Duration: {duration}\")\n",
    "        \n",
    "        if sync.final_status.message:\n",
    "            logger.info(f\"   Message: {sync.final_status.message}\")\n",
    "    \n",
    "    logger.info(\"=\" * 80)\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to fetch sync history: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3174101e-3bcd-4d64-aa30-e66d7d8d1825",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)\n",
    "\n",
    "### Delete Feature Online Store\n",
    "\n",
    "**WARNING**: This will permanently delete the Feature Online Store and all associated data.  \n",
    "Uncomment and run the cell below only if you want to remove the resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8da35f-4915-434f-82c5-00501fbe2ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to delete the Feature Online Store\n",
    "# try:\n",
    "#     logger.info(f\"Deleting Feature Online Store: {FEATURE_ONLINE_STORE_ID}\")\n",
    "#     logger.info(\"This may take several minutes...\")\n",
    "#     \n",
    "#     delete_op = admin_vfs2_client.delete_feature_online_store(\n",
    "#         name=f\"projects/{PROJECT_ID}/locations/{REGION_ID}/featureOnlineStores/{FEATURE_ONLINE_STORE_ID}\",\n",
    "#         force=True,\n",
    "#     )\n",
    "#     \n",
    "#     delete_op.result()\n",
    "#     logger.info(\"✓ Feature Online Store deleted successfully\")\n",
    "#     \n",
    "# except exceptions.NotFound:\n",
    "#     logger.warning(\"Feature Online Store not found (may already be deleted)\")\n",
    "# except Exception as e:\n",
    "#     logger.error(f\"Failed to delete Feature Online Store: {e}\")\n",
    "#     raise"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m114",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/:m114"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}